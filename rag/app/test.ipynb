{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2eb6a1-e737-4da4-a2c8-67e7a6369d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[0;93m2025-07-15 15:35:41.311055205 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-07-15 15:35:41.311097628 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    readed = [0] * len(paper[\"lines\"])\\n    # find colon firstly\\n    i = 0\\n    while i + 1 < len(paper[\"lines\"]):\\n        txt = pdf_parser.remove_tag(paper[\"lines\"][i][0])\\n        j = i\\n        if txt.strip(\"\\n\").strip()[-1] not in \":：\":\\n            i += 1\\n            continue\\n        i += 1\\n        while i < len(paper[\"lines\"]) and not paper[\"lines\"][i][0]:\\n            i += 1\\n        if i >= len(paper[\"lines\"]): break\\n        proj = [paper[\"lines\"][i][0].strip()]\\n        i += 1\\n        while i < len(paper[\"lines\"]) and paper[\"lines\"][i][0].strip()[0] == proj[-1][0]:\\n            proj.append(paper[\"lines\"][i])\\n            i += 1\\n        for k in range(j, i): readed[k] = True\\n        txt = txt[::-1]\\n        if eng:\\n            r = re.search(r\"(.*?) ([\\\\.;?!]|$)\", txt)\\n            txt = r.group(1)[::-1] if r else txt[::-1]\\n        else:\\n            r = re.search(r\"(.*?) ([。？；！]|$)\", txt)\\n            txt = r.group(1)[::-1] if r else txt[::-1]\\n        for p in proj:\\n            d = copy.deepcopy(doc)\\n            txt += \"\\n\" + pdf_parser.remove_tag(p)\\n            d[\"image\"], poss = pdf_parser.crop(p, need_position=True)\\n            add_positions(d, poss)\\n            tokenize(d, txt, eng)\\n            res.append(d)\\n\\n    i = 0\\n    chunk = []\\n    tk_cnt = 0\\n    def add_chunk():\\n        nonlocal chunk, res, doc, pdf_parser, tk_cnt\\n        d = copy.deepcopy(doc)\\n        ck = \"\\n\".join(chunk)\\n        tokenize(d, pdf_parser.remove_tag(ck), pdf_parser.is_english)\\n        d[\"image\"], poss = pdf_parser.crop(ck, need_position=True)\\n        add_positions(d, poss)\\n        res.append(d)\\n        chunk = []\\n        tk_cnt = 0\\n\\n    while i < len(paper[\"lines\"]):\\n        if tk_cnt > 128:\\n            add_chunk()\\n        if readed[i]:\\n            i += 1\\n            continue\\n        readed[i] = True\\n        txt, layouts = paper[\"lines\"][i]\\n        txt_ = pdf_parser.remove_tag(txt)\\n        i += 1\\n        cnt = num_tokens_from_string(txt_)\\n        if any([\\n            layouts.find(\"title\") >= 0 and chunk,\\n            cnt + tk_cnt > 128 and tk_cnt > 32,\\n        ]):\\n            add_chunk()\\n            chunk = [txt]\\n            tk_cnt = cnt\\n        else:\\n            chunk.append(txt)\\n            tk_cnt += cnt\\n\\n    if chunk: add_chunk()\\n    for i, d in enumerate(res):\\n        print(d)\\n        # d[\"image\"].save(f\"./logs/{i}.jpg\")\\n    return res\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "#  Copyright 2025 The InfiniFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "#\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "# sys.path.append(\"/root/ragflow\")\n",
    "from api.db import ParserType\n",
    "from rag.nlp import rag_tokenizer, tokenize, tokenize_table, add_positions, bullets_category, title_frequency, tokenize_chunks\n",
    "from deepdoc.parser import PdfParser, PlainParser\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Pdf(PdfParser):\n",
    "    def __init__(self):\n",
    "        self.model_speciess = ParserType.PAPER.value\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, filename, binary=None, from_page=0,\n",
    "                 to_page=100000, zoomin=3, callback=None):\n",
    "        from timeit import default_timer as timer\n",
    "        start = timer()\n",
    "        callback(msg=\"OCR started\")\n",
    "        self.__images__(\n",
    "            filename if not binary else binary,\n",
    "            zoomin,\n",
    "            from_page,\n",
    "            to_page,\n",
    "            callback\n",
    "        )\n",
    "        callback(msg=\"OCR finished ({:.2f}s)\".format(timer() - start))\n",
    "\n",
    "        start = timer()\n",
    "        self._layouts_rec(zoomin)\n",
    "        callback(0.63, \"Layout analysis ({:.2f}s)\".format(timer() - start))\n",
    "        logging.debug(f\"layouts cost: {timer() - start}s\")\n",
    "\n",
    "        start = timer()\n",
    "        self._table_transformer_job(zoomin)\n",
    "        callback(0.68, \"Table analysis ({:.2f}s)\".format(timer() - start))\n",
    "\n",
    "        start = timer()\n",
    "        self._text_merge()\n",
    "        tbls = self._extract_table_figure(True, zoomin, True, True)\n",
    "        column_width = np.median([b[\"x1\"] - b[\"x0\"] for b in self.boxes])\n",
    "        self._concat_downward()\n",
    "        self._filter_forpages()\n",
    "        callback(0.75, \"Text merged ({:.2f}s)\".format(timer() - start))\n",
    "\n",
    "        # clean mess\n",
    "        if column_width < self.page_images[0].size[0] / zoomin / 2:\n",
    "            logging.debug(\"two_column................... {} {}\".format(column_width,\n",
    "                  self.page_images[0].size[0] / zoomin / 2))\n",
    "            self.boxes = self.sort_X_by_page(self.boxes, column_width / 2)\n",
    "        for b in self.boxes:\n",
    "            b[\"text\"] = re.sub(r\"([\\t 　]|\\u3000){2,}\", \" \", b[\"text\"].strip())\n",
    "\n",
    "        def _begin(txt):\n",
    "            return re.match(\n",
    "                \"[0-9. 一、i]*(introduction|abstract|摘要|引言|keywords|key words|关键词|background|背景|目录|前言|contents)\",\n",
    "                txt.lower().strip())\n",
    "\n",
    "        if from_page > 0:\n",
    "            return {\n",
    "                \"title\": \"\",\n",
    "                \"authors\": \"\",\n",
    "                \"abstract\": \"\",\n",
    "                \"sections\": [(b[\"text\"] + self._line_tag(b, zoomin), b.get(\"layoutno\", \"\")) for b in self.boxes if\n",
    "                             re.match(r\"(text|title)\", b.get(\"layoutno\", \"text\"))],\n",
    "                \"tables\": tbls\n",
    "            }\n",
    "        # get title and authors\n",
    "        title = \"\"\n",
    "        authors = []\n",
    "        i = 0\n",
    "        while i < min(32, len(self.boxes)-1):\n",
    "            b = self.boxes[i]\n",
    "            i += 1\n",
    "            if b.get(\"layoutno\", \"\").find(\"title\") >= 0:\n",
    "                title = b[\"text\"]\n",
    "                if _begin(title):\n",
    "                    title = \"\"\n",
    "                    break\n",
    "                for j in range(3):\n",
    "                    if _begin(self.boxes[i + j][\"text\"]):\n",
    "                        break\n",
    "                    authors.append(self.boxes[i + j][\"text\"])\n",
    "                    break\n",
    "                break\n",
    "        # get abstract\n",
    "        abstr = \"\"\n",
    "        i = 0\n",
    "        while i + 1 < min(32, len(self.boxes)):\n",
    "            b = self.boxes[i]\n",
    "            i += 1\n",
    "            txt = b[\"text\"].lower().strip()\n",
    "            if re.match(\"(abstract|摘要)\", txt):\n",
    "                if len(txt.split()) > 32 or len(txt) > 64:\n",
    "                    abstr = txt + self._line_tag(b, zoomin)\n",
    "                    break\n",
    "                txt = self.boxes[i][\"text\"].lower().strip()\n",
    "                if len(txt.split()) > 32 or len(txt) > 64:\n",
    "                    abstr = txt + self._line_tag(self.boxes[i], zoomin)\n",
    "                i += 1\n",
    "                break\n",
    "        if not abstr:\n",
    "            i = 0\n",
    "\n",
    "        callback(\n",
    "            0.8, \"Page {}~{}: Text merging finished\".format(\n",
    "                from_page, min(\n",
    "                    to_page, self.total_page)))\n",
    "        for b in self.boxes:\n",
    "            logging.debug(\"{} {}\".format(b[\"text\"], b.get(\"layoutno\")))\n",
    "        logging.debug(\"{}\".format(tbls))\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"authors\": \" \".join(authors),\n",
    "            \"abstract\": abstr,\n",
    "            \"sections\": [(b[\"text\"] + self._line_tag(b, zoomin), b.get(\"layoutno\", \"\")) for b in self.boxes[i:] if\n",
    "                         re.match(r\"(text|title)\", b.get(\"layoutno\", \"text\"))],\n",
    "            \"tables\": tbls\n",
    "        }\n",
    "\n",
    "\n",
    "def chunk(filename, binary=None, from_page=0, to_page=100000,\n",
    "          lang=\"Chinese\", callback=None, **kwargs):\n",
    "    \"\"\"\n",
    "        Only pdf is supported.\n",
    "        The abstract of the paper will be sliced as an entire chunk, and will not be sliced partly.\n",
    "    \"\"\"\n",
    "    if re.search(r\"\\.pdf$\", filename, re.IGNORECASE):\n",
    "        if kwargs.get(\"parser_config\", {}).get(\"layout_recognize\", \"DeepDOC\") == \"Plain Text\":\n",
    "            pdf_parser = PlainParser()\n",
    "            paper = {\n",
    "                \"title\": filename,\n",
    "                \"authors\": \" \",\n",
    "                \"abstract\": \"\",\n",
    "                \"sections\": pdf_parser(filename if not binary else binary, from_page=from_page, to_page=to_page)[0],\n",
    "                \"tables\": []\n",
    "            }\n",
    "        else:\n",
    "            pdf_parser = Pdf()\n",
    "            paper = pdf_parser(filename if not binary else binary,\n",
    "                               from_page=from_page, to_page=to_page, callback=callback)\n",
    "    else:\n",
    "        raise NotImplementedError(\"file type not supported yet(pdf supported)\")\n",
    "\n",
    "    doc = {\"docnm_kwd\": filename, \"authors_tks\": rag_tokenizer.tokenize(paper[\"authors\"]),\n",
    "           \"title_tks\": rag_tokenizer.tokenize(paper[\"title\"] if paper[\"title\"] else filename)}\n",
    "    doc[\"title_sm_tks\"] = rag_tokenizer.fine_grained_tokenize(doc[\"title_tks\"])\n",
    "    doc[\"authors_sm_tks\"] = rag_tokenizer.fine_grained_tokenize(doc[\"authors_tks\"])\n",
    "    # is it English\n",
    "    eng = lang.lower() == \"english\"  # pdf_parser.is_english\n",
    "    logging.debug(\"It's English.....{}\".format(eng))\n",
    "\n",
    "    res = tokenize_table(paper[\"tables\"], doc, eng)\n",
    "\n",
    "    if paper[\"abstract\"]:\n",
    "        d = copy.deepcopy(doc)\n",
    "        txt = pdf_parser.remove_tag(paper[\"abstract\"])\n",
    "        d[\"important_kwd\"] = [\"abstract\", \"总结\", \"概括\", \"summary\", \"summarize\"]\n",
    "        d[\"important_tks\"] = \" \".join(d[\"important_kwd\"])\n",
    "        d[\"image\"], poss = pdf_parser.crop(\n",
    "            paper[\"abstract\"], need_position=True)\n",
    "        add_positions(d, poss)\n",
    "        tokenize(d, txt, eng)\n",
    "        res.append(d)\n",
    "\n",
    "    sorted_sections = paper[\"sections\"]\n",
    "    # set pivot using the most frequent type of title,\n",
    "    # then merge between 2 pivot\n",
    "    bull = bullets_category([txt for txt, _ in sorted_sections])\n",
    "    most_level, levels = title_frequency(bull, sorted_sections)\n",
    "    assert len(sorted_sections) == len(levels)\n",
    "    sec_ids = []\n",
    "    sid = 0\n",
    "    for i, lvl in enumerate(levels):\n",
    "        if lvl <= most_level and i > 0 and lvl != levels[i - 1]:\n",
    "            sid += 1\n",
    "        sec_ids.append(sid)\n",
    "        logging.debug(\"{} {} {} {}\".format(lvl, sorted_sections[i][0], most_level, sid))\n",
    "\n",
    "    chunks = []\n",
    "    last_sid = -2\n",
    "    for (txt, _), sec_id in zip(sorted_sections, sec_ids):\n",
    "        if sec_id == last_sid:\n",
    "            if chunks:\n",
    "                chunks[-1] += \"\\n\" + txt\n",
    "                continue\n",
    "        chunks.append(txt)\n",
    "        last_sid = sec_id\n",
    "    res.extend(tokenize_chunks(chunks, doc, eng, pdf_parser))\n",
    "    return res\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    readed = [0] * len(paper[\"lines\"])\n",
    "    # find colon firstly\n",
    "    i = 0\n",
    "    while i + 1 < len(paper[\"lines\"]):\n",
    "        txt = pdf_parser.remove_tag(paper[\"lines\"][i][0])\n",
    "        j = i\n",
    "        if txt.strip(\"\\n\").strip()[-1] not in \":：\":\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        while i < len(paper[\"lines\"]) and not paper[\"lines\"][i][0]:\n",
    "            i += 1\n",
    "        if i >= len(paper[\"lines\"]): break\n",
    "        proj = [paper[\"lines\"][i][0].strip()]\n",
    "        i += 1\n",
    "        while i < len(paper[\"lines\"]) and paper[\"lines\"][i][0].strip()[0] == proj[-1][0]:\n",
    "            proj.append(paper[\"lines\"][i])\n",
    "            i += 1\n",
    "        for k in range(j, i): readed[k] = True\n",
    "        txt = txt[::-1]\n",
    "        if eng:\n",
    "            r = re.search(r\"(.*?) ([\\\\.;?!]|$)\", txt)\n",
    "            txt = r.group(1)[::-1] if r else txt[::-1]\n",
    "        else:\n",
    "            r = re.search(r\"(.*?) ([。？；！]|$)\", txt)\n",
    "            txt = r.group(1)[::-1] if r else txt[::-1]\n",
    "        for p in proj:\n",
    "            d = copy.deepcopy(doc)\n",
    "            txt += \"\\n\" + pdf_parser.remove_tag(p)\n",
    "            d[\"image\"], poss = pdf_parser.crop(p, need_position=True)\n",
    "            add_positions(d, poss)\n",
    "            tokenize(d, txt, eng)\n",
    "            res.append(d)\n",
    "\n",
    "    i = 0\n",
    "    chunk = []\n",
    "    tk_cnt = 0\n",
    "    def add_chunk():\n",
    "        nonlocal chunk, res, doc, pdf_parser, tk_cnt\n",
    "        d = copy.deepcopy(doc)\n",
    "        ck = \"\\n\".join(chunk)\n",
    "        tokenize(d, pdf_parser.remove_tag(ck), pdf_parser.is_english)\n",
    "        d[\"image\"], poss = pdf_parser.crop(ck, need_position=True)\n",
    "        add_positions(d, poss)\n",
    "        res.append(d)\n",
    "        chunk = []\n",
    "        tk_cnt = 0\n",
    "\n",
    "    while i < len(paper[\"lines\"]):\n",
    "        if tk_cnt > 128:\n",
    "            add_chunk()\n",
    "        if readed[i]:\n",
    "            i += 1\n",
    "            continue\n",
    "        readed[i] = True\n",
    "        txt, layouts = paper[\"lines\"][i]\n",
    "        txt_ = pdf_parser.remove_tag(txt)\n",
    "        i += 1\n",
    "        cnt = num_tokens_from_string(txt_)\n",
    "        if any([\n",
    "            layouts.find(\"title\") >= 0 and chunk,\n",
    "            cnt + tk_cnt > 128 and tk_cnt > 32,\n",
    "        ]):\n",
    "            add_chunk()\n",
    "            chunk = [txt]\n",
    "            tk_cnt = cnt\n",
    "        else:\n",
    "            chunk.append(txt)\n",
    "            tk_cnt += cnt\n",
    "\n",
    "    if chunk: add_chunk()\n",
    "    for i, d in enumerate(res):\n",
    "        print(d)\n",
    "        # d[\"image\"].save(f\"./logs/{i}.jpg\")\n",
    "    return res\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5c6235-eb4a-4d05-9dcc-8aebc2692d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_paths= ['/root/docs//rope旋转位置编码.pdf']\n",
      "/root/docs//rope旋转位置编码.pdf\n",
      "第0path= /root/docs//rope旋转位置编码.pdf\n",
      "None OCR started\n",
      "0.2571428571428571 \n",
      "0.5142857142857142 \n",
      "None OCR finished (107.45s)\n",
      "0.63 Layout analysis (43.18s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing layout match: 2,figure caption-0\n",
      "WARNING:root:Missing layout match: 3,equation-0\n",
      "WARNING:root:Missing layout match: 3,figure caption-2\n",
      "WARNING:root:Missing layout match: 3,equation-2\n",
      "WARNING:root:Missing layout match: 3,equation-3\n",
      "WARNING:root:Missing layout match: 3,figure caption-4\n",
      "WARNING:root:Missing layout match: 3,figure caption-6\n",
      "WARNING:root:Missing layout match: 3,equation-6\n",
      "WARNING:root:Missing layout match: 3,figure caption-7\n",
      "WARNING:root:Missing layout match: 4,figure caption-0\n",
      "WARNING:root:Missing layout match: 4,figure caption-1\n",
      "WARNING:root:Missing layout match: 4,equation-2\n",
      "WARNING:root:Missing layout match: 4,figure caption-3\n",
      "WARNING:root:Missing layout match: 5,equation-0\n",
      "WARNING:root:Missing layout match: 5,equation-2\n",
      "WARNING:root:Missing layout match: 6,figure caption-0\n",
      "WARNING:root:Missing layout match: 6,figure caption-1\n",
      "WARNING:root:Missing layout match: 6,figure caption-2\n",
      "WARNING:root:Missing layout match: 6,equation-3\n",
      "WARNING:root:Missing layout match: 6,figure caption-5\n",
      "WARNING:root:Missing layout match: 6,equation-5\n",
      "WARNING:root:Missing layout match: 6,figure caption-7\n",
      "WARNING:root:Missing layout match: 6,equation-7\n",
      "WARNING:root:Missing layout match: 7,equation-0\n",
      "WARNING:root:Missing layout match: 7,figure caption-3\n",
      "WARNING:root:Missing layout match: 7,equation-2\n",
      "WARNING:root:Missing layout match: 7,equation-3\n",
      "WARNING:root:Missing layout match: 7,figure caption-6\n",
      "WARNING:root:Missing layout match: 7,equation-5\n",
      "WARNING:root:Missing layout match: 7,figure caption-8\n",
      "WARNING:root:Missing layout match: 7,equation-7\n",
      "WARNING:root:Missing layout match: 7,equation-8\n",
      "WARNING:root:Missing layout match: 7,figure caption-10\n",
      "WARNING:root:Missing layout match: 8,equation-0\n",
      "WARNING:root:Missing layout match: 8,figure caption-2\n",
      "WARNING:root:Missing layout match: 8,figure caption-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68 Table analysis (2.64s)\n",
      "0.75 Text merged (5.11s)\n",
      "0.8 Page 0~14: Text merging finished\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def dummy(prog=None, msg=\"\"):\n",
    "    print(prog,msg)\n",
    "#chunk(sys.argv[1], callback=dummy)\n",
    "import os\n",
    "path = \"/root/docs/\"\n",
    "files=os.listdir(path)\n",
    "file_paths=[path+\"/\"+file for file in files]\n",
    "print(\"file_paths=\",file_paths)\n",
    "for i in file_paths:\n",
    "    print(i)\n",
    "all_chunks = []\n",
    "# 遍历处理每个文件\n",
    "for i,path in enumerate(file_paths):\n",
    "    print(f\"第{i}path=\",path)\n",
    "    chunks = chunk(path, callback=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8e79d7-3e21-43f6-9d48-c756544ff0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(1)\\nqm =fg(cm,m)\\nkn=fk(Cn,n)\\nUn=f(Cn,n),',\n",
       "  'content_ltks': '1 qm fg cm m kn fk cn n un f cn n',\n",
       "  'content_sm_ltks': '1 qm fg cm m kn fk cn n un f cn n',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=823x121>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [2],\n",
       "  'position_int': [(2, 267, 542, 648, 689)],\n",
       "  'top_int': [648]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'exp(nm)\\n/d\\nam,n\\n√d\\nN\\n0m=\\n_am,nUn\\nn=1',\n",
       "  'content_ltks': 'exp nm d am n d n 0m _ am nun n 1',\n",
       "  'content_sm_ltks': 'exp nm d am n d n 0m _ am nun n 1',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=290x211>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 252, 349, 84, 155)],\n",
       "  'top_int': [84]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(4)\\n(3)\\n(2)\\nft:tE{q,k,u}(ci, i) := Wt:te{q,k,u}(ci + Pi),',\n",
       "  'content_ltks': '43 2 ft te q k u ci i wt te q k u ci pi',\n",
       "  'content_sm_ltks': '43 2 ft te q k u ci i wt te q k u ci pi',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=982x604>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 214, 542, 114, 316)],\n",
       "  'top_int': [114]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '= sin(k /100002t/d)\\nPi,2t\\n(Pi,2t+1 = cos(k /100002t/d)',\n",
       "  'content_ltks': 'sin k 100002t d pi 2t pi 2t 1 co k 100002t d',\n",
       "  'content_sm_ltks': 'sin k 100002t d pi 2t pi 2t 1 co k 100002t d',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=386x84>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 241, 369, 298, 326)],\n",
       "  'top_int': [298]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'fa(αm) := Wqm\\nfk(xn,n) :=Wk(n +pk)\\nfu(cn,n) := Wu(n +p)',\n",
       "  'content_ltks': 'fa α m wqm fk xn n wk n pk fu cn n wu n p',\n",
       "  'content_sm_ltks': 'fa α m wqm fk xn n wk n pk fu cn n wu n p',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=360x137>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 246, 366, 421, 467)],\n",
       "  'top_int': [421]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(6)\\n(5)\\nqmkn = 2m WIWkn + cm WIWkPn + pm WIWkcn + pm WIWkPn,',\n",
       "  'content_ltks': '65 qmkn 2m wiwkn cm wiwkpn pm wiwkcn pm wiwkpn',\n",
       "  'content_sm_ltks': '65 qmkn 2m wiwkn cm wiwkpn pm wiwkcn pm wiwkpn',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1189x287>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 145, 542, 439, 535)],\n",
       "  'top_int': [439]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(8)\\n(7)\\nqmkn = cm WIWkn + cm WIWkPm-n + uTWIWkn + vTWIWkPm-n',\n",
       "  'content_ltks': '8 7 qmkn cm wiwkn cm wiwkpm n utwiwkn vtwiwkpm n',\n",
       "  'content_sm_ltks': '8 7 qmkn cm wiwkn cm wiwkpm n utwiwkn vtwiwkpm n',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1219x237>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 135, 542, 591, 670)],\n",
       "  'top_int': [591]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'qmkn = cmWIWkn + bi,j',\n",
       "  'content_ltks': 'qmkn cmwiwkn bi j',\n",
       "  'content_sm_ltks': 'qmkn cmwiwkn bi j',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=384x42>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 241, 369, 658, 672)],\n",
       "  'top_int': [658]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(9)\\nqmkn = cm WIWkCn + pm UIUkPn + bi,j',\n",
       "  'content_ltks': '9 qmkn cm wiwkcn pm uiukpn bi j',\n",
       "  'content_sm_ltks': '9 qmkn cm wiwkcn pm uiukpn bi j',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=992x37>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [3],\n",
       "  'position_int': [(3, 211, 542, 712, 724)],\n",
       "  'top_int': [712]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(10)\\nqmkn = cm WWkn + xmWWkPm-n +Pm-nWWkn',\n",
       "  'content_ltks': '10 qmkn cm wwkn xmwwkpm n pm nwwkn',\n",
       "  'content_sm_ltks': '10 qmkn cm wwkn xmwwkpm n pm nwwkn',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1115x40>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [4],\n",
       "  'position_int': [(4, 171, 543, 116, 129)],\n",
       "  'top_int': [116]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(11)\\n(fg(xm, m), fk(cn, n) = g(cm, Wn, m - n).',\n",
       "  'content_ltks': '11 fg xm m fk cn n g cm wn m n',\n",
       "  'content_sm_ltks': '11 fg xm m fk cn n g cm wn m n',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=992x42>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [4],\n",
       "  'position_int': [(4, 211, 542, 401, 415)],\n",
       "  'top_int': [401]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'fa(Em,m) =(WqaEm)eim6\\nfk(cn, n) = (WkCn)einθ\\ng(Mm, Cn, m - n) = Re[(WqMm)(Wkcn)*ei(m-n)θ]',\n",
       "  'content_ltks': 'fa em m wqaem eim6 fk cn n wkcn ein θ g mm cn m n re wqmm wkcn ei m n θ',\n",
       "  'content_sm_ltks': 'fa em m wqaem eim6 fk cn n wkcn ein θ g mm cn m n re wqmm wkcn ei m n θ',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=678x146>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [4],\n",
       "  'position_int': [(4, 192, 418, 550, 599)],\n",
       "  'top_int': [550]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(13)\\n(12)\\ncm\\ncOs mθ\\n-sin mθ\\ngk}\\nf{q,k}(cm,m) =\\nsin mθ\\ncos mθ\\n21)',\n",
       "  'content_ltks': '13 12 cm co m θ sin m θ gk f q k cm m sin m θ co m θ 21',\n",
       "  'content_sm_ltks': '13 12 cm co m θ sin m θ gk f q k cm m sin m θ co m θ 21',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1180x284>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [4],\n",
       "  'position_int': [(4, 149, 543, 567, 662)],\n",
       "  'top_int': [567]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'f(q,k}(Cm, m) = R,mW{g,k}Cm',\n",
       "  'content_ltks': 'f q k cm m r mw g k cm',\n",
       "  'content_sm_ltks': 'f q k cm m r mw g k cm',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=440x49>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [5],\n",
       "  'position_int': [(5, 232, 379, 131, 147)],\n",
       "  'top_int': [131]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'qmkn = (Rb,m Wqm)T(R,n Wkcn) = xTWgR,n-mWkCn',\n",
       "  'content_ltks': 'qmkn rb m wqm t r n wkcn xtwgr n mwkcn',\n",
       "  'content_sm_ltks': 'qmkn rb m wqm t r n wkcn xtwgr n mwkcn',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=830x50>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [5],\n",
       "  'position_int': [(5, 167, 444, 283, 300)],\n",
       "  'top_int': [283]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': \"Figure 1: Implementation of Rotary Position Embedding(RoPE).\\nm0\\nx'2\\nX2\\n(X1,X2)\\n(x'1,x'2)\\nQuery / Key\\nx'1\\nX1\\nm.\\nd=2\\n02\\n..\\nEnhanced\\nTransforr\\nrmer\\nwith\\n··\\nRotary …·\\n··\\nPosition\\nEmbedding\\n：：\\n6\\nPosition\\nPosition Er\\ncodedQuery/Key\\n/Key\",\n",
       "  'content_ltks': 'figur 1 implement of rotari posit embed rope m0 x 2 x2 x1 x2 x 1x 2 queri key x 1 x1 m d2 02 enhanc transforr rmer with rotari posit embed 6 posit posit er codedqueri key key',\n",
       "  'content_sm_ltks': 'figur 1 implement of rotari posit embed rope m0 x 2 x2 x1 x2 x 1x 2 queri key x 1 x1 m d2 02 enhanc transforr rmer with rotari posit embed 6 posit posit er codedqueri key key',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=993x555>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [5],\n",
       "  'position_int': [(5, 139, 470, 402, 587)],\n",
       "  'top_int': [402]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(17)\\nAttention(Q, K, V)m =\\n∑n=1 sim(qm,kn)',\n",
       "  'content_ltks': '17 attent q k v m n 1 sim qm kn',\n",
       "  'content_sm_ltks': '17 attent q k v m n 1 sim qm kn',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1012x71>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 204, 542, 91, 115)],\n",
       "  'top_int': [91]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(18)\\n∑n=1 Φ(qm)Tq(kn)Un\\nAttention(Q, K, V)m =\\n∑n=1 Φ(qm)T(kn)',\n",
       "  'content_ltks': '18 n 1 φ qm tq kn un attent q k v m n 1 φ qm t kn',\n",
       "  'content_sm_ltks': '18 n 1 φ qm tq kn un attent q k v m n 1 φ qm t kn',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1027x105>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 200, 543, 160, 195)],\n",
       "  'top_int': [160]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(19)\\n∑n=1 (R,mΦ(qm)T(R,n(kn)Un \\nAttention(Q,K, V)m =\\n∑n=1 Φ(qm)Tp(kn)',\n",
       "  'content_ltks': '19 n 1 r m φ qm t r n kn un attent q k v m n 1 φ qm tp kn',\n",
       "  'content_sm_ltks': '19 n 1 r m φ qm t r n kn un attent q k v m n 1 φ qm tp kn',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1117x106>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 169, 542, 295, 330)],\n",
       "  'top_int': [295]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'Qm = fa(wq, m),\\nkn = fk(ck,n),',\n",
       "  'content_ltks': 'qm fa wq m kn fk ck n',\n",
       "  'content_sm_ltks': 'qm fa wq m kn fk ck n',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=224x76>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 267, 342, 449, 474)],\n",
       "  'top_int': [449]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(22)\\n(21)\\n(20)\\nqmkn =<fq(αm,m), fk(cn,n)) = g(cm,Mn, n- m),',\n",
       "  'content_ltks': '22 21 20 qmkn fq α m m fk cn n g cm mn n m',\n",
       "  'content_sm_ltks': '22 21 20 qmkn fq α m m fk cn n g cm mn n m',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1050x319>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 193, 543, 455, 561)],\n",
       "  'top_int': [455]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'q= fa(∞q,0),\\nk=fk(k,O),',\n",
       "  'content_ltks': 'q fa q 0 k fk k o',\n",
       "  'content_sm_ltks': 'q fa q 0 k fk k o',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=190x76>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 273, 337, 542, 567)],\n",
       "  'top_int': [542]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(24)\\n(23)\\nf(xq, m) = Rg(xq,m)ei0q(mq,m),\\nfk(ck,n) = Rk(ak,n)eiOk(mk,n),\\ng(xq,k, n - m) = Rg(q,k,n -m)erg(mg,k,n-m),',\n",
       "  'content_ltks': '24 23 f xq m rg xq m ei0q mq m fk ck n rk ak n eiok mk n g xq k n m rg q k n m erg mg k n m',\n",
       "  'content_sm_ltks': '24 23 f xq m rg xq m ei0q mq m fk ck n rk ak n eiok mk n g xq k n m rg q k n m erg mg k n m',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1062x319>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 188, 542, 613, 719)],\n",
       "  'top_int': [613]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'Rq(q,m)Rk(k,n) = Rg(q,k,n - m),\\nk(k,n) -q(q,m) =g(q,k,n-m),',\n",
       "  'content_ltks': 'rq qm rk k n rg q k n m k k n q qm g q k n m',\n",
       "  'content_sm_ltks': 'rq qm rk k n rg q k n m k k n q qm g q k n m',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=578x77>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [6],\n",
       "  'position_int': [(6, 209, 402, 700, 726)],\n",
       "  'top_int': [700]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'q=Illeiθa = Rg(xq, 0)eig(zq,0),\\nk=I/kllei0 = Rk(ck,0)ei0k(mx,0),',\n",
       "  'content_ltks': 'q illei θ a rg xq 0 eig zq 0 k i kllei0 rk ck 0 ei0k mx 0',\n",
       "  'content_sm_ltks': 'q illei θ a rg xq 0 eig zq 0 k i kllei0 rk ck 0 ei0k mx 0',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=459x101>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 227, 380, 91, 124)],\n",
       "  'top_int': [91]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(27)\\n(26b)\\n(26a)\\n(25)\\nRq(q, m)Rk(ck,m) = Rg(cq,k, 0) = Rq(xq, 0)Rk(ck,0) = Ilalllkl,\\nOk(k,m) - θq(q,m) = θg(q,k,0) = k(k,0) -θq(∞q,0) = 0k - 0q',\n",
       "  'content_ltks': '27 26b 26a 25 rq qm rk ck m rg cq k 0 rq xq 0 rk ck 0 ilalllkl ok k m θ q qm θ g q k 0 k k 0 θ q q 0 0k 0q',\n",
       "  'content_sm_ltks': '27 26b 26a 25 rq qm rk ck m rg cq k 0 rq xq 0 rk ck 0 ilalllkl ok k m θ q qm θ g q k 0 k k 0 θ q q 0 0k 0q',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1202x413>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 142, 543, 100, 238)],\n",
       "  'top_int': [100]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'Rg(∞q, m) = Rg(xq, 0) = Ilall\\nRk(ck,n) = Rk(ck,0) = lkll\\nRg(q, Ck, n- m) = Rg(xq,ck, 0) = Ilalllkll',\n",
       "  'content_ltks': 'rg qm rg xq 0 ilal rk ck n rk ck 0 lkll rg q ck n m rg xq ck 0 ilalllkl',\n",
       "  'content_sm_ltks': 'rg qm rg xq 0 ilal rk ck n rk ck 0 lkll rg q ck n m rg xq ck 0 ilalllkl',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=584x126>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 208, 403, 213, 255)],\n",
       "  'top_int': [213]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '日f(x{g,k},m) = Φ(m) + 0(g,k},',\n",
       "  'content_ltks': '日 f x g k m φ m 0 g k',\n",
       "  'content_sm_ltks': '日 f x g k m φ m 0 g k',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=414x50>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 236, 374, 310, 326)],\n",
       "  'top_int': [310]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(30)\\n(29)\\n(28)\\nΦ(m + 1) - Φ(m) = θg(q,∞k,1) + 0q - 0k,',\n",
       "  'content_ltks': '30 29 28 φ m1 φ m θ g q k 1 0q 0k',\n",
       "  'content_sm_ltks': '30 29 28 φ m1 φ m θ g q k 1 0q 0k',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=995x254>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 211, 543, 312, 396)],\n",
       "  'top_int': [312]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'Φ(m) = mθ +,',\n",
       "  'content_ltks': 'φ m m θ',\n",
       "  'content_sm_ltks': 'φ m m θ',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=212x34>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 271, 341, 385, 396)],\n",
       "  'top_int': [385]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(32)\\n(31)\\nfa(aq, m) = Ille@a+m0+ = ge(m0+),\\nfk:(ck,n) =I/kllei0k+n0+ = ke’(n0+r).',\n",
       "  'content_ltks': '32 31 fa aq m ill a m0 ge m0 fk ck n i kllei0k n0 ke n0 r',\n",
       "  'content_sm_ltks': '32 31 fa aq m ill a m0 ge m0 fk ck n i kllei0k n0 ke n0 r',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=973x262>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 218, 543, 428, 515)],\n",
       "  'top_int': [428]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'q= fa(cm,0) =Wqn,\\nk = fk(cn, 0) = WkCn.',\n",
       "  'content_ltks': 'q fa cm 0 wqn k fk cn 0 wkcn',\n",
       "  'content_sm_ltks': 'q fa cm 0 wqn k fk cn 0 wkcn',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=322x85>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 252, 359, 495, 524)],\n",
       "  'top_int': [495]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'fa(xm,m) =(Wqzm)eime,\\nfk:(cn, n) = (Wkxn)einθ',\n",
       "  'content_ltks': 'fa xm m wqzm eim fk cn n wkxn ein θ',\n",
       "  'content_sm_ltks': 'fa xm m wqzm eim fk cn n wkxn ein θ',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=361x94>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 244, 364, 547, 578)],\n",
       "  'top_int': [547]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(34)\\n(33)\\n—C2\\ncos m01\\n sin m01\\nsin m01\\ncOs m01\\nC2\\nC1\\ncos m02\\n-C4\\nsin m02\\nC3\\nsin m02\\nRd.mc=\\ncOS m02\\nC4\\nC3\\n十\\n:\\n:\\n：\\ncos m日 d/2\\nsin m0d/2\\nxd-1\\n-Cd\\n[cosm0a/2\\n(sin m0a/2\\nMd\\n(cd-1/',\n",
       "  'content_ltks': '34 33 c2 co m01 sin m01 sin m01 co m01 c2 c1 co m02 c4 sin m02 c3 sin m02 rd mc co m02 c4 c3 十 co m 日 d2 sin m0d 2 xd 1 cd cosm0a 2 sin m0a 2 md cd 1',\n",
       "  'content_sm_ltks': '34 33 c2 co m01 sin m01 sin m01 co m01 c2 c1 co m02 c4 sin m02 c3 sin m02 rd mc co m02 c4 c3 十 co m 日 d2 sin m0d 2 xd 1 cd cosm0a 2 sin m0a 2 md cd 1',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1123x507>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [7],\n",
       "  'position_int': [(7, 168, 543, 555, 724)],\n",
       "  'top_int': [555]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'Figure 2: Long-term decay of RoPE.\\nrelativeupperbound\\n20\\nrelativedistance\\n100\\n150\\n200\\n250\\n50',\n",
       "  'content_ltks': 'figur 2 long term decay of rope relativeupperbound 20 relativedist 100 150 200 250 50',\n",
       "  'content_sm_ltks': 'figur 2 long term decay of rope relativeupperbound 20 relativedist 100 150 200 250 50',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=968x519>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [8],\n",
       "  'position_int': [(8, 148, 471, 69, 242)],\n",
       "  'top_int': [69]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'd/2-1\\n(R,m Wqm)T(R,nWkn) = Re\\nq[2i:2i+\\n=0',\n",
       "  'content_ltks': 'd2 1 r m wqm t r nwkn re q 2i 2i 0',\n",
       "  'content_sm_ltks': 'd2 1 r m wqm t r nwkn re q 2i 2i 0',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=650x97>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [8],\n",
       "  'position_int': [(8, 153, 370, 340, 373)],\n",
       "  'top_int': [340]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(36)\\n(35)\\nd/2-1\\nd/2-1\\nd/2-1\\ni(m-n)0i\\nSi+1(hi+1 - hi).\\nQ[2i:2i+1]k[2i:2i+1]\\nhi(Si+1- Si):\\ni=0\\ni=0\\n=0',\n",
       "  'content_ltks': '36 35 d2 1 d2 1 d2 1 i m n 0i si 1 hi 1 hi q 2i 2i 1 k 2i 2i 1 hi si 1 si i 0 i 0 0',\n",
       "  'content_sm_ltks': '36 35 d2 1 d2 1 d2 1 i m n 0i si 1 hi 1 hi q 2i 2i 1 k 2i 2i 1 hi si 1 si i 0 i 0 0',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1235x307>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [8],\n",
       "  'position_int': [(8, 131, 543, 351, 453)],\n",
       "  'top_int': [351]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '(37)\\nd/2-\\ni(m\\nSi+1(hi+1\\nQ[2i:2i+1][2i:2i+\\ni=0\\nd/2-1\\n√\\n[Si+ill(hi+1 -hi)l\\ni=0\\nd/2-1\\n≤ISi+1l\\n≤ (max |hi+1 \\nhi1\\ni=0',\n",
       "  'content_ltks': '37 d2 i m si 1 hi 1 q 2i 2i 1 2i 2i i 0 d2 1 si ill hi 1 hi l i 0 d2 1 isi 1l max hi 1 hi1 i 0',\n",
       "  'content_sm_ltks': '37 d2 i m si 1 hi 1 q 2i 2i 1 2i 2i i 0 d2 1 si ill hi 1 hi l i 0 d2 1 isi 1l max hi 1 hi1 i 0',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1061x314>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [8],\n",
       "  'position_int': [(8, 189, 543, 491, 595)],\n",
       "  'top_int': [491]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': 'training loss for PerFormer with and without RoPE.\\n Figure 3: Evaluation of RoPE in language modeling pre-training. Left: training loss for BERT and RoFormer. Right:\\nPerFormer w/. RoPE\\nRoFor\\nBERT\\nPerFormerw/o.RoPE\\nSOT\\nMLM\\n1.5\\n100\\n150\\n200\\n50\\n60\\n250\\n40\\n80\\n100\\nTrain Steps (K)\\nTrain Steps (K)',\n",
       "  'content_ltks': 'train loss for perform with and without rope figur 3 evalu of rope in languag model pre train left train loss for bert and roform right perform w rope rofor bert performerw o rope sot mlm 15 100 150 200 50 60 250 40 80 100 train step k train step k',\n",
       "  'content_sm_ltks': 'train loss for perform with and without rope figur 3 evalu of rope in languag model pre train left train loss for bert and roform right perform w rope rofor bert performerw o rope sot mlm 15 100 150 200 50 60 250 40 80 100 train step k train step k',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1270x407>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [10],\n",
       "  'position_int': [(10, 90, 513, 86, 222)],\n",
       "  'top_int': [86]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '<table><caption>(14)(15)(16)</caption>\\n<tr><td  >/cos m01</td><td  >- sin m01</td><td  >0</td><td  >0</td><td></td><td  >0</td><td  >0</td></tr>\\n<tr><td  >sin m01</td><td  >cos m01</td><td  >0</td><td  >0</td><td  >..</td><td  >0</td><td  >0</td></tr>\\n<tr><td  >0</td><td  >0</td><td  >cos m02</td><td  >sin m02</td><td></td><td  >0</td><td  >0</td></tr>\\n<tr><td  >0</td><td  >0</td><td  >sin m02</td><td  >cOS m02</td><td></td><td  >0</td><td  >0</td></tr>\\n<tr><th  >R</th><th  >:</th><th></th><th></th><th  >.</th><th></th><th></th></tr>\\n<tr><td  >0</td><td  >0</td><td  >0</td><td  >0</td><td></td><td  > cos m0 a/2</td><td  >- sin m0a/2</td></tr>\\n<tr><td  >0</td><td  >0</td><td  >0</td><td  >0</td><td></td><td  >sin m0a/2</td><td  >cos m日a/2</td></tr>\\n</table>',\n",
       "  'content_ltks': '14 15 16 co m01 sin m01 0 0 0 0 sin m01 co m01 0 0 0 0 0 0 co m02 sin m02 0 0 0 0 sin m02 co m02 0 0 r 0 0 0 0 co m0 a2 sin m0a 2000 0 sin m0a 2 co m 日 a2',\n",
       "  'content_sm_ltks': '14 15 16 co m01 sin m01 0 0 0 0 sin m01 co m01 0 0 0 0 0 0 co m02 sin m02 0 0 0 0 sin m02 co m02 0 0 r 0 0 0 0 co m0 a2 sin m0a 2000 0 sin m0a 2 co m 日 a2',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1099x262>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [5],\n",
       "  'position_int': [(5, 121, 488, 160, 247)],\n",
       "  'top_int': [160]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '<table>\\n<tr><th  >Model</th><th  >BLEU</th></tr>\\n<tr><td  >Transformer-base Vaswani et al. [2017]</td><td  >27.3</td></tr>\\n<tr><td></td><td  >27.5</td></tr>\\n</table>',\n",
       "  'content_ltks': 'model bleu transform base vaswani et al 2017 273 27 5',\n",
       "  'content_sm_ltks': 'model bleu transform base vaswani et al 2017 273 27 5',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=619x137>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [9],\n",
       "  'position_int': [(9, 202, 409, 98, 144)],\n",
       "  'top_int': [98]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '<table><caption>Table 2: Comparing RoFormer and BERT by fine tuning on downstream GLEU tasks.</caption>\\n<tr><th  > Model</th><th  >MRPC</th><th  >SST-2</th><th  >QNLI</th><th  >STS-B</th><th  >QQP</th><th  >MNLI(m/mm)</th></tr>\\n<tr><td  >BERTDevlin et al. [2019]</td><td  >88.9</td><td  >93.5</td><td  >90.5</td><td  >85.8</td><td  >71.2</td><td  >84.6/83.4</td></tr>\\n<tr><td></td><td  >89.5</td><td  >90.7</td><td  >88.0</td><td  >87.0</td><td  >86.4</td><td  >80.2/79.8</td></tr>\\n</table>',\n",
       "  'content_ltks': 'tabl 2 compar roform and bert by finetune on downstream gleu task model mrpc sst 2 qnli st b qqp mnli m mm bertdevlin et al 2019 88 9 935 90 5 85 8 71 2 84 6 83 489 5 90 7 88 0 87 0 86 4 80 279 8',\n",
       "  'content_sm_ltks': 'tabl 2 compar roform and bert by finetune on downstream gleu task model mrpc sst 2 qnli st b qqp mnli m mm bertdevlin et al 2019 88 9 935 90 5 85 8 71 2 84 6 83 489 5 90 7 88 0 87 0 86 4 80 279 8',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1107x140>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [10],\n",
       "  'position_int': [(10, 121, 490, 505, 552)],\n",
       "  'top_int': [505]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': \"<table><caption>Table 3: Cross-comparison between our RoFormer and other pre-trained models on Chinese data. 'abs’ and 'rel' annotates absolute position embedding and relative position embedding, respectively.</caption>\\n<tr><td  >Model</td><td  >BERTDevlin et al. [2019]  WoBERTSu [2020]</td><td></td><td  >NEZHA Wei et al. [2019]</td><td></td></tr>\\n<tr><td  >Tokenization level</td><td  >char</td><td  >word</td><td  >char</td><td  >word</td></tr>\\n<tr><td  >Position embedding</td><td  >abs.</td><td  > abs.</td><td  >rel.</td><td  >RoPE</td></tr>\\n</table>\",\n",
       "  'content_ltks': 'tabl 3 cross comparison between our roform and other pre train model on chines data ab and rel annot absolut posit embed and rel posit embed respect model bertdevlin et al 2019 wobertsu 2020 nezha wei et al 2019 token level char word char word posit embed ab ab rel rope',\n",
       "  'content_sm_ltks': 'tabl 3 cross comparison between our roform and other pre train model on chines data ab and rel annot absolut posit embed and rel posit embed respect model bertdevlin et al 2019 wobertsu 2020 nezha wei et al 2019 token level char word char word posit embed ab ab rel rope',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1394x139>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [11],\n",
       "  'position_int': [(11, 73, 538, 378, 425)],\n",
       "  'top_int': [378]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '<table>\\n<tr><th  >Stage</th><th  >Max seq length</th><th  >Batch size</th><th  >Training steps</th><th  >Loss</th><th  >Accuracy</th></tr>\\n<tr><td  >1</td><td  >512</td><td  >256</td><td  >200k</td><td  >1.73</td><td  >65.0%</td></tr>\\n<tr><td></td><td  >1536</td><td  >256</td><td  >12.5k</td><td  >1.61</td><td  >66.8%</td></tr>\\n<tr><td></td><td  >256</td><td  >256</td><td  >120k</td><td  >1.75</td><td  >64.6%</td></tr>\\n<tr><td></td><td  >128</td><td  >512</td><td  >80k</td><td  >1.83</td><td  >63.4%</td></tr>\\n<tr><td></td><td  >1536</td><td  >256</td><td  >10k</td><td  >1.58</td><td  >67.4%</td></tr>\\n<tr><td  >6</td><td  >512</td><td  >512</td><td  >30k</td><td  >1.66</td><td  >66.2%</td></tr>\\n</table>',\n",
       "  'content_ltks': 'stage max seq length batch size train step loss accuraci 1 512 256 200k 173 65 0 1536 256 12 5k 161 668 256 256 120k 1 75 64 6 128 512 80k 183 63 4 1536 256 10k 158 67 4 6 512 512 30k 166 66 2',\n",
       "  'content_sm_ltks': 'stage max seq length batch size train step loss accuraci 1 512 256 200k 173 65 0 1536 256 12 5k 161 668 256 256 120k 1 75 64 6 128 512 80k 183 63 4 1536 256 10k 158 67 4 6 512 512 30k 166 66 2',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=928x267>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [11],\n",
       "  'position_int': [(11, 151, 461, 555, 644)],\n",
       "  'top_int': [555]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'content_with_weight': '<table>\\n<tr><th  >Model</th><th  >Validation</th><th  >Test</th></tr>\\n<tr><td  >BERT-512</td><td  >64.13%</td><td  >67.77%</td></tr>\\n<tr><td  >WoBERT-512</td><td  >64.07%</td><td  >68.10%</td></tr>\\n<tr><td  >RoFormer-512</td><td  >64.13%</td><td  >68.29%</td></tr>\\n<tr><td  >RoFormer-102466.07%</td><td></td><td  >69.79%</td></tr>\\n</table>',\n",
       "  'content_ltks': 'model valid test bert 512 64 13 67 77 wobert 512 64 07 68 10 roform 512 64 13 68 29 roform 102466 07 69 79',\n",
       "  'content_sm_ltks': 'model valid test bert 512 64 13 67 77 wobert 512 64 07 68 10 roform 512 64 13 68 29 roform 102466 07 69 79',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=533x203>,\n",
       "  'doc_type_kwd': 'image',\n",
       "  'page_num_int': [12],\n",
       "  'position_int': [(12, 216, 394, 251, 319)],\n",
       "  'top_int': [251]},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'important_kwd': ['abstract', '总结', '概括', 'summary', 'summarize'],\n",
       "  'important_tks': 'abstract 总结 概括 summary summarize',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1192x734>,\n",
       "  'page_num_int': [1],\n",
       "  'position_int': [(1, 107, 504, 366, 377)],\n",
       "  'top_int': [366],\n",
       "  'content_with_weight': 'position encoding recently has shown effective in the transformer architecture. it enables valuable',\n",
       "  'content_ltks': 'posit encod recent ha shown effect in the transform architectur it enabl valuabl',\n",
       "  'content_sm_ltks': 'posit encod recent ha shown effect in the transform architectur it enabl valuabl'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1417x3450>,\n",
       "  'page_num_int': [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2],\n",
       "  'position_int': [(1, 107, 579, 377, 388),\n",
       "   (1, 104, 577, 386, 400),\n",
       "   (1, 107, 579, 399, 411),\n",
       "   (1, 105, 578, 407, 421),\n",
       "   (1, 107, 579, 420, 432),\n",
       "   (1, 107, 579, 431, 443),\n",
       "   (1, 106, 578, 441, 455),\n",
       "   (1, 106, 578, 452, 466),\n",
       "   (1, 106, 578, 464, 476),\n",
       "   (1, 107, 579, 475, 486),\n",
       "   (1, 106, 578, 485, 499),\n",
       "   (1, 106, 578, 497, 509),\n",
       "   (1, 105, 578, 507, 521),\n",
       "   (1, 69, 541, 528, 543),\n",
       "   (1, 69, 541, 555, 569),\n",
       "   (1, 70, 542, 580, 594),\n",
       "   (1, 71, 544, 594, 605),\n",
       "   (1, 71, 544, 604, 616),\n",
       "   (1, 71, 544, 615, 627),\n",
       "   (1, 71, 544, 627, 638),\n",
       "   (1, 70, 543, 637, 651),\n",
       "   (1, 70, 543, 647, 661),\n",
       "   (1, 71, 544, 660, 670),\n",
       "   (1, 71, 544, 670, 682),\n",
       "   (1, 71, 544, 681, 693),\n",
       "   (1, 70, 543, 693, 704),\n",
       "   (1, 83, 555, 710, 724),\n",
       "   (2, 70, 543, 74, 85),\n",
       "   (2, 71, 544, 85, 97),\n",
       "   (2, 70, 543, 94, 108),\n",
       "   (2, 70, 543, 105, 117),\n",
       "   (2, 71, 544, 118, 128),\n",
       "   (2, 70, 542, 126, 140),\n",
       "   (2, 71, 544, 139, 151),\n",
       "   (2, 71, 544, 151, 161),\n",
       "   (2, 71, 544, 161, 173),\n",
       "   (2, 69, 542, 171, 185),\n",
       "   (2, 70, 542, 182, 196),\n",
       "   (2, 71, 544, 194, 206),\n",
       "   (2, 70, 542, 209, 223),\n",
       "   (2, 71, 544, 222, 233),\n",
       "   (2, 70, 543, 232, 244),\n",
       "   (2, 71, 544, 243, 255),\n",
       "   (2, 70, 542, 252, 266),\n",
       "   (2, 71, 544, 265, 277),\n",
       "   (2, 71, 544, 275, 287),\n",
       "   (2, 70, 543, 287, 298),\n",
       "   (2, 71, 544, 303, 314),\n",
       "   (2, 98, 570, 326, 338),\n",
       "   (2, 106, 578, 337, 349),\n",
       "   (2, 107, 579, 349, 359),\n",
       "   (2, 106, 578, 358, 372),\n",
       "   (2, 107, 579, 370, 382),\n",
       "   (2, 99, 571, 390, 401),\n",
       "   (2, 105, 578, 399, 413),\n",
       "   (2, 107, 579, 411, 423),\n",
       "   (2, 99, 571, 432, 444),\n",
       "   (2, 108, 580, 444, 455),\n",
       "   (2, 107, 579, 454, 466),\n",
       "   (2, 70, 542, 474, 488),\n",
       "   (2, 70, 543, 486, 498),\n",
       "   (2, 70, 542, 496, 510),\n",
       "   (2, 70, 542, 508, 519),\n",
       "   (2, 70, 542, 543, 554)],\n",
       "  'top_int': [377,\n",
       "   386,\n",
       "   399,\n",
       "   407,\n",
       "   420,\n",
       "   431,\n",
       "   441,\n",
       "   452,\n",
       "   464,\n",
       "   475,\n",
       "   485,\n",
       "   497,\n",
       "   507,\n",
       "   528,\n",
       "   555,\n",
       "   580,\n",
       "   594,\n",
       "   604,\n",
       "   615,\n",
       "   627,\n",
       "   637,\n",
       "   647,\n",
       "   660,\n",
       "   670,\n",
       "   681,\n",
       "   693,\n",
       "   710,\n",
       "   74,\n",
       "   85,\n",
       "   94,\n",
       "   105,\n",
       "   118,\n",
       "   126,\n",
       "   139,\n",
       "   151,\n",
       "   161,\n",
       "   171,\n",
       "   182,\n",
       "   194,\n",
       "   209,\n",
       "   222,\n",
       "   232,\n",
       "   243,\n",
       "   252,\n",
       "   265,\n",
       "   275,\n",
       "   287,\n",
       "   303,\n",
       "   326,\n",
       "   337,\n",
       "   349,\n",
       "   358,\n",
       "   370,\n",
       "   390,\n",
       "   399,\n",
       "   411,\n",
       "   432,\n",
       "   444,\n",
       "   454,\n",
       "   474,\n",
       "   486,\n",
       "   496,\n",
       "   508,\n",
       "   543],\n",
       "  'content_with_weight': 'supervision for dependency modeling between elements at different positions of the sequence. In\\nthis paper, we first investigate various methods to integrate positional information into the learning\\nprocess of transformer-based language models. Then, we propose a novel method named Rotary\\nPosition Embedding(RoPE) to effectively leverage the positional information. Specifically, the\\nproposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates\\nthe explicit relative position dependency in self-attention formulation. Notably, RoPE enables\\nvaluable properties, including the flexibility of sequence length, decaying inter-token dependency\\nwith increasing relative distances, and the capability of equipping the linear self-attention with\\nrelative position encoding. Finally, we evaluate the enhanced transformer with rotary position\\nembedding, also called RoFormer, on various long text classification benchmark datasets. Our\\nexperiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical\\nanalysis to explain some experimental results. RoFormer is already integrated into Huggingface:\\nhttps : //huggingface.co/docs/transformers/model_doc/roformer.\\nKeywords Pre-trained Language Models · Position Information Encoding Pre-training · Natural Language Processing.\\n1Introduction\\nThe sequential order of words is of great value to natural language understanding. Recurrent neural networks (RRNs)\\nbased models encode tokens? order by recursively computing a hidden state along the time dimension. Convolution\\nneural networks (CNNs) based models (CNNs) Gehring et al. [2017] were typically considered position-agnostic, but\\nrecent work Islam et al. [2020] has shown that the commonly used padding operation can implicitly learn position\\ninformation. Recently, the pre-trained language models (PLMs), which were built upon the transformer Vaswani et al.\\n[2017], have achieved the state-of-the-art performance of various natural language processing (NLP) tasks, including\\ncontext representation learning Devlin et al. [2019], machine translation Vaswani et al. [2017], and language modeling\\nRadford et al. [2019], to name a few. Unlike, RRNs and CNNs-based models, PLMs utilize the self-attention mechanism\\nto semantically capture the contextual representation of a given corpus. As a consequence, PLMs achieve a significant\\nimprovement in terms of parallelization over RNNs and improve the modeling ability of longer intra-token relations\\ncompared to CNNs*.\\n1A stack of multiple CNN layers can also capture longer intra-token relation, here we only consider single layer setting.\\nIt is noteworthy that the self-attention architecture of the current PLMs has shown to be position-agnostic Yun et al.\\n[2020]. Following this claim, various approaches have been proposed to encode the position information into the\\nlearning process. On one side, generated absolute position encoding through a pre-defined function Vaswani et al.\\n[2017] was added to the contextual representations, while a trainable absolute position encoding Gehring et al. [2017],\\nDevlin et al. [2019], Lan et al. [2020], Clark et al. [2020], Radford et al. [2019], Radford and Narasimhan [2018]. On\\nthe other side, the previous work Parikh et al. [2016], Shaw et al. [2018], Huang et al. [2018], Dai et al. [2019], Yang\\net al. [2019], Raffel et al. [2020], Ke et al. [2020], He et al. [2020], Huang et al. [2020] focuses 0n relative position\\nencoding, which typically encodes the relative position information into the attention mechanism. In addition to these\\napproaches, the authors of Liu et al. [2020] have proposed to model the dependency of position encoding from the\\nperspective of Neural ODE Chen et al. [2018a], and the authors of Wang et al. [2020] have proposed to model the\\nposition information in complex space. Despite the effectiveness of these approaches, they commonly add the position\\ninformation to the context representation and thus render them unsuitable for the linear self-attention architecture.\\nIn this paper, we introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional\\ninformation into the learning process of PLMS. Specifically, RoPE encodes the absolute position with a rotation matrix\\nand meanwhile incorporates the explicit relative position dependency in self-attention formulation. Note that the\\nproposed RoPE is prioritized over the existing methods through valuable properties, including the sequence length\\nflexibility, decaying inter-token dependency with increasing relative distances, and the capability of equipping the\\nlinear self-attention with relative position encoding. Experimental results on various long text classification benchmark\\ndatasets show that the enhanced transformer with rotary position embedding, namely RoFormer, can give better\\nperformance compared to baseline alternatives and thus demonstrates the efficacy of the proposed RoPE.\\nIn brief, our contributions are three-folds as follows:\\n· We investigated the existing approaches to the relative position encoding and found that they are mostly\\nbuilt based on the idea of the decomposition of adding position encoding to the context representations. We\\nintroduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information\\ninto the learning proces of PLMS. The key idea is to encode relative position by multiplying the context\\nrepresentations with a rotation matrix with a clear theoretical interpretation.\\n· We study the properties of RoPE and show that it decays with the relative distance increased, which is desired\\nfor natural language encoding. We kindly argue that previous relative position encoding-based approaches are\\nnot compatible with linear self-attention.\\n· We evaluate the proposed RoFormer on various long text benchmark datasets. Our experiments show that it\\nconsistently achieves better performance compared to its alternatives. Some experiments with pre-trained\\nlanguage models are available on GitHub: https : //github. com/ZhuiyiTechnology/roformer.\\nThe remaining of the paper is organized as follows. We establish a formal description of the position encoding problem\\nin self-attention architecture and revisit previous works in Section (2). We then describe the rotary position encoding\\n(RoPE) and study its properties in Section (3). We report experiments in Section (4). Finally, we conclude this paper in\\nSection (5).\\n2Background and Related Work',\n",
       "  'content_ltks': 'supervis for depend model between element at differ posit of the sequenc in thi paper we first investig variou method to integr posit inform into the learn process of transform base languag model then we propos a novel method name rotari posit embed rope to effect leverag the posit inform specif the propos rope encod the absolut posit with a rotat matrix and meanwhil incorpor the explicit rel posit depend in self attent formul notabl rope enabl valuabl properti includ the flexibl of sequenc length decay inter token depend with increas rel distanc and the capabl of equip the linear self attent with rel posit encod final we evalu the enhanc transform with rotari posit embed also call roform on variou long text classif benchmark dataset our experi show that it consist overcom it altern furthermor we provid a theoret analysi to explain some experiment result roform is alreadi integr into huggingfac http huggingfac co doc transform model _ doc roform keyword pre train languag model posit inform encod pre train natur languag process 1introduct the sequenti order of word is of great valu to natur languag understand recurr neural network rrn base model encod token order by recurs comput a hidden state along the time dimens convolut neural network cnn base model cnn gehr et al 2017 were typic consid posit agnost but recent work islam et al 2020 ha shown that the commonli use pad oper can implicitli learn posit inform recent the pre train languag model plm which were built upon the transform vaswani et al 2017 have achiev the state of the art perform of variou natur languag process nlp task includ context represent learn devlin et al 2019 machin translat vaswani et al 2017 and languag model radford et al 2019 to name a few unlik rrn and cnn base model plm util the self attent mechan to semant captur the contextu represent of a given corpu a a consequ plm achiev a signific improv in term of parallel over rnn and improv the model abil of longer intra token relat compar to cnn 1a stack of multipl cnn layer can also captur longer intra token relat here we onli consid singl layer set it is noteworthi that the self attent architectur of the current plm ha shown to be posit agnost yun et al 2020 follow thi claim variou approach have been propos to encod the posit inform into the learn process on one side gener absolut posit encod through a pre defin function vaswani et al 2017 wa ad to the contextu represent while a trainabl absolut posit encod gehr et al 2017 devlin et al 2019 lan et al 2020 clark et al 2020 radford et al 2019 radford and narasimhan 2018 on the other side the previou work parikh et al 2016 shaw et al 2018 huang et al 2018 dai et al 2019 yang et al 2019 raffel et al 2020 ke et al 2020 he et al 2020 huang et al 2020 focu 0n rel posit encod which typic encod the rel posit inform into the attent mechan in addit to these approach the author of liu et al 2020 have propos to model the depend of posit encod from the perspect of neural ode chen et al 2018a and the author of wang et al 2020 have propos to model the posit inform in complex space despit the effect of these approach they commonli add the posit inform to the context represent and thu render them unsuit for the linear self attent architectur in thi paper we introduc a novel method name rotari posit embed rope to leverag the posit inform into the learn process of plm specif rope encod the absolut posit with a rotat matrix and meanwhil incorpor the explicit rel posit depend in self attent formul note that the propos rope is priorit over the exist method through valuabl properti includ the sequenc length flexibl decay inter token depend with increas rel distanc and the capabl of equip the linear self attent with rel posit encod experiment result on variou long text classif benchmark dataset show that the enhanc transform with rotari posit embed name roform can give better perform compar to baselin altern and thu demonstr the efficaci of the propos rope in brief our contribut are three fold a follow we investig the exist approach to the rel posit encod and found that they are mostli built base on the idea of the decomposit of ad posit encod to the context represent we introduc a novel method name rotari posit embed rope to leverag the posit inform into the learn proce of plm the key idea is to encod rel posit by multipli the context represent with a rotat matrix with a clear theoret interpret we studi the properti of rope and show that it decay with the rel distanc increas which is desir for natur languag encod we kindli argu that previou rel posit encod base approach are not compat with linear self attent we evalu the propos roform on variou long text benchmark dataset our experi show that it consist achiev better perform compar to it altern some experi with pre train languag model are avail on github http github com zhuiyitechnolog roform the remain of the paper is organ a follow we establish a formal descript of the posit encod problem in self attent architectur and revisit previou work in section 2 we then describ the rotari posit encod rope and studi it properti in section 3 we report experi in section 4 final we conclud thi paper in section 5 2background and relat work',\n",
       "  'content_sm_ltks': 'supervis for depend model between element at differ posit of the sequenc in thi paper we first investig variou method to integr posit inform into the learn process of transform base languag model then we propos a novel method name rotari posit embed rope to effect leverag the posit inform specif the propos rope encod the absolut posit with a rotat matrix and meanwhil incorpor the explicit rel posit depend in self attent formul notabl rope enabl valuabl properti includ the flexibl of sequenc length decay inter token depend with increas rel distanc and the capabl of equip the linear self attent with rel posit encod final we evalu the enhanc transform with rotari posit embed also call roform on variou long text classif benchmark dataset our experi show that it consist overcom it altern furthermor we provid a theoret analysi to explain some experiment result roform is alreadi integr into huggingfac http huggingfac co doc transform model _ doc roform keyword pre train languag model posit inform encod pre train natur languag process 1introduct the sequenti order of word is of great valu to natur languag understand recurr neural network rrn base model encod token order by recurs comput a hidden state along the time dimens convolut neural network cnn base model cnn gehr et al 2017 were typic consid posit agnost but recent work islam et al 2020 ha shown that the commonli use pad oper can implicitli learn posit inform recent the pre train languag model plm which were built upon the transform vaswani et al 2017 have achiev the state of the art perform of variou natur languag process nlp task includ context represent learn devlin et al 2019 machin translat vaswani et al 2017 and languag model radford et al 2019 to name a few unlik rrn and cnn base model plm util the self attent mechan to semant captur the contextu represent of a given corpu a a consequ plm achiev a signific improv in term of parallel over rnn and improv the model abil of longer intra token relat compar to cnn 1a stack of multipl cnn layer can also captur longer intra token relat here we onli consid singl layer set it is noteworthi that the self attent architectur of the current plm ha shown to be posit agnost yun et al 2020 follow thi claim variou approach have been propos to encod the posit inform into the learn process on one side gener absolut posit encod through a pre defin function vaswani et al 2017 wa ad to the contextu represent while a trainabl absolut posit encod gehr et al 2017 devlin et al 2019 lan et al 2020 clark et al 2020 radford et al 2019 radford and narasimhan 2018 on the other side the previou work parikh et al 2016 shaw et al 2018 huang et al 2018 dai et al 2019 yang et al 2019 raffel et al 2020 ke et al 2020 he et al 2020 huang et al 2020 focu 0n rel posit encod which typic encod the rel posit inform into the attent mechan in addit to these approach the author of liu et al 2020 have propos to model the depend of posit encod from the perspect of neural ode chen et al 2018a and the author of wang et al 2020 have propos to model the posit inform in complex space despit the effect of these approach they commonli add the posit inform to the context represent and thu render them unsuit for the linear self attent architectur in thi paper we introduc a novel method name rotari posit embed rope to leverag the posit inform into the learn process of plm specif rope encod the absolut posit with a rotat matrix and meanwhil incorpor the explicit rel posit depend in self attent formul note that the propos rope is priorit over the exist method through valuabl properti includ the sequenc length flexibl decay inter token depend with increas rel distanc and the capabl of equip the linear self attent with rel posit encod experiment result on variou long text classif benchmark dataset show that the enhanc transform with rotari posit embed name roform can give better perform compar to baselin altern and thu demonstr the efficaci of the propos rope in brief our contribut are three fold a follow we investig the exist approach to the rel posit encod and found that they are mostli built base on the idea of the decomposit of ad posit encod to the context represent we introduc a novel method name rotari posit embed rope to leverag the posit inform into the learn proce of plm the key idea is to encod rel posit by multipli the context represent with a rotat matrix with a clear theoret interpret we studi the properti of rope and show that it decay with the rel distanc increas which is desir for natur languag encod we kindli argu that previou rel posit encod base approach are not compat with linear self attent we evalu the propos roform on variou long text benchmark dataset our experi show that it consist achiev better perform compar to it altern some experi with pre train languag model are avail on github http github com zhuiyitechnolog roform the remain of the paper is organ a follow we establish a formal descript of the posit encod problem in self attent architectur and revisit previou work in section 2 we then describ the rotari posit encod rope and studi it properti in section 3 we report experi in section 4 final we conclud thi paper in section 5 2background and relat work'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1420x1147>,\n",
       "  'page_num_int': [2, 2, 2, 2, 2, 2, 2, 3, 3, 3],\n",
       "  'position_int': [(2, 70, 543, 569, 580),\n",
       "   (2, 70, 543, 590, 604),\n",
       "   (2, 69, 542, 600, 617),\n",
       "   (2, 71, 545, 615, 627),\n",
       "   (2, 70, 544, 625, 637),\n",
       "   (2, 70, 544, 700, 714),\n",
       "   (2, 70, 543, 711, 725),\n",
       "   (3, 70, 544, 74, 85),\n",
       "   (3, 69, 542, 156, 170),\n",
       "   (3, 70, 544, 169, 180)],\n",
       "  'top_int': [569, 590, 600, 615, 625, 700, 711, 74, 156, 169],\n",
       "  'content_with_weight': '2.1Preliminary\\nLet SN = {w}1 be a sequence of N input tokens with w being the ith element. The corresponding word embedding\\nof S is denoted as EN = {i1, where c; E Rd is the d-dimensional word embedding vector of token w; without\\nposition information. The self-attention first incorporates position information to the word embeddings and transforms\\nthem into queries, keys, and value representations.\\nwhere qm, kn and vn incorporate the mth and nth positions through fq, fk and fu, respectively. The query and key\\nvalues are then used to compute the attention weights, while the output is computed as the weighted sum over the value\\nrepresentation.\\nThe existing approaches of transformer-based position encoding mainly focus on choosing a suitable function to form\\nEquation (1).',\n",
       "  'content_ltks': '2 1preliminari let sn w 1 be a sequenc of n input token with w be the ith element the correspond word embed of s is denot a en i1 where c e rd is the d dimension word embed vector of token w without posit inform the self attent first incorpor posit inform to the word embed and transform them into queri key and valu represent where qm kn and vn incorpor the mth and nth posit through fq fk and fu respect the queri and key valu are then use to comput the attent weight while the output is comput a the weight sum over the valu represent the exist approach of transform base posit encod mainli focu on choos a suitabl function to form equat 1',\n",
       "  'content_sm_ltks': '2 1preliminari let sn w 1 be a sequenc of n input token with w be the ith element the correspond word embed of s is denot a en i1 where c e rd is the d dimension word embed vector of token w without posit inform the self attent first incorpor posit inform to the word embed and transform them into queri key and valu represent where qm kn and vn incorpor the mth and nth posit through fq fk and fu respect the queri and key valu are then use to comput the attent weight while the output is comput a the weight sum over the valu represent the exist approach of transform base posit encod mainli focu on choos a suitabl function to form equat 1'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1416x1126>,\n",
       "  'page_num_int': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "  'position_int': [(3, 70, 542, 191, 203),\n",
       "   (3, 70, 542, 213, 224),\n",
       "   (3, 70, 542, 246, 260),\n",
       "   (3, 70, 542, 258, 269),\n",
       "   (3, 71, 543, 269, 280),\n",
       "   (3, 70, 542, 280, 292),\n",
       "   (3, 70, 542, 329, 343),\n",
       "   (3, 70, 542, 340, 354),\n",
       "   (3, 70, 542, 353, 364),\n",
       "   (3, 70, 542, 363, 375)],\n",
       "  'top_int': [191, 213, 246, 258, 269, 280, 329, 340, 353, 363],\n",
       "  'content_with_weight': '2.2 Absolute position embedding\\nA typical choice of Equation (1) is\\nwhere p, E Rd is a d-dimensional vector depending of the position of token 2. Previous work Devlin et al. [2019],\\nLan et al. [2020], Clark et al. [2020], Radford et al. [2019], Radford and Narasimhan [2018] introduced the use of a set\\nof trainable vectors p E {pt}t=1, where L is the maximum sequence length. The authors of Vaswani et al. [2017] have\\nproposed to generate p using the sinusoidal function.\\nin which pi,2t is the 2tth element of the d-dimensional vector p. In the next section, we show that our proposed RoPE\\nis related to this intuition from the sinusoidal function perspective. However, instead of directly adding the position\\nto the context representation, RoPE proposes to incorporate the relative position information by multiplying with the\\nsinusoidal functions.',\n",
       "  'content_ltks': '2 2 absolut posit embed a typic choic of equat 1 is where p e rd is a d dimension vector depend of the posit of token 2 previou work devlin et al 2019 lan et al 2020 clark et al 2020 radford et al 2019 radford and narasimhan 2018 introduc the use of a set of trainabl vector p e pt t 1 where l is the maximum sequenc length the author of vaswani et al 2017 have propos to gener p use the sinusoid function in which pi 2t is the 2tth element of the d dimension vector p in the next section we show that our propos rope is relat to thi intuit from the sinusoid function perspect howev instead of directli ad the posit to the context represent rope propos to incorpor the rel posit inform by multipli with the sinusoid function',\n",
       "  'content_sm_ltks': '2 2 absolut posit embed a typic choic of equat 1 is where p e rd is a d dimension vector depend of the posit of token 2 previou work devlin et al 2019 lan et al 2020 clark et al 2020 radford et al 2019 radford and narasimhan 2018 introduc the use of a set of trainabl vector p e pt t 1 where l is the maximum sequenc length the author of vaswani et al 2017 have propos to gener p use the sinusoid function in which pi 2t is the 2tth element of the d dimension vector p in the next section we show that our propos rope is relat to thi intuit from the sinusoid function perspect howev instead of directli ad the posit to the context represent rope propos to incorpor the rel posit inform by multipli with the sinusoid function'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1424x1989>,\n",
       "  'page_num_int': [3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4,\n",
       "   4],\n",
       "  'position_int': [(3, 70, 544, 387, 399),\n",
       "   (3, 69, 543, 406, 420),\n",
       "   (3, 69, 543, 471, 484),\n",
       "   (3, 71, 546, 484, 495),\n",
       "   (3, 71, 546, 495, 506),\n",
       "   (3, 70, 544, 504, 518),\n",
       "   (3, 68, 543, 535, 552),\n",
       "   (3, 71, 546, 550, 561),\n",
       "   (3, 71, 546, 561, 572),\n",
       "   (3, 70, 545, 573, 585),\n",
       "   (3, 70, 545, 616, 627),\n",
       "   (3, 70, 544, 626, 638),\n",
       "   (3, 70, 545, 637, 649),\n",
       "   (3, 70, 545, 649, 660),\n",
       "   (3, 71, 546, 674, 685),\n",
       "   (3, 70, 544, 683, 697),\n",
       "   (3, 71, 546, 696, 708),\n",
       "   (4, 70, 545, 74, 85),\n",
       "   (4, 71, 546, 85, 97),\n",
       "   (4, 69, 543, 93, 108),\n",
       "   (4, 71, 546, 137, 149),\n",
       "   (4, 70, 545, 148, 160),\n",
       "   (4, 70, 544, 158, 172),\n",
       "   (4, 71, 546, 170, 182),\n",
       "   (4, 71, 546, 181, 193),\n",
       "   (4, 71, 546, 192, 203),\n",
       "   (4, 71, 546, 203, 215),\n",
       "   (4, 69, 543, 231, 245),\n",
       "   (4, 71, 546, 259, 270),\n",
       "   (4, 70, 545, 269, 281)],\n",
       "  'top_int': [387,\n",
       "   406,\n",
       "   471,\n",
       "   484,\n",
       "   495,\n",
       "   504,\n",
       "   535,\n",
       "   550,\n",
       "   561,\n",
       "   573,\n",
       "   616,\n",
       "   626,\n",
       "   637,\n",
       "   649,\n",
       "   674,\n",
       "   683,\n",
       "   696,\n",
       "   74,\n",
       "   85,\n",
       "   93,\n",
       "   137,\n",
       "   148,\n",
       "   158,\n",
       "   170,\n",
       "   181,\n",
       "   192,\n",
       "   203,\n",
       "   231,\n",
       "   259,\n",
       "   269],\n",
       "  'content_with_weight': \"2.3 Relative position embedding\\nThe authors of Shaw et al. [2018] applied different settings of Equation (1) as following:\\nwhere p, p E Rd are trainable relative position embeddings. Note that r = clip(m - n, r'min, 'max) represents the\\nrelative distance between position m and n. They clipped the relative distance with the hypothesis that precise relative\\nposition information is not useful beyond a certain distance. Keeping the form of Equation (3), the authors Dai et al.\\n[2019] have proposed to decompose qmkn of Equation (2) as\\nthe key idea is to replace the absolute position embedding pn with its sinusoid-encoded relative counterpart pm -n*\\nwhile the absolute position Pm in the third and fourth term with two trainable vectors u and v independent of the query\\npositions. Further, W is distinguished for the content-based and location-based key vectors n and pn, denoted as\\nWk and W k, resulting in:\\nIt is noteworthy that the position information in the value term is removed by setting f(§) := W ,§. Later work\\nRaffel et al. [2020], He et al. [2020], Ke et al. [2020], Huang et al. [2020] followed these settings by only encoding\\nthe relative position information into the attention weights. However, the authors of Raffel et al. [2020] reformed\\nEquation (6) as:\\nwhere b,j is a trainable bias. The authors of Ke et al. [2020] investigated the middle two terms of Equation (6) and\\nfound little correlations between absolute positions and words. The authors of Raffel et al. [2020] proposed to model a\\npair of words or positions using different projection matrices.\\nThe authors of He et al. [2020] argued that the relative positions of two tokens could only be fully modeled using\\nthe middle two terms of Equation (6). As a consequence, the absolute position embeddings Pm and pn were simply\\nreplaced with the relative position embeddings Pm-n:\\nA comparison of the four variants of the relative position embeddings Radford and Narasimhan [2018] has shown\\nthat the variant similar to Equation (1o) is the most efficient among the other three. Generally speaking, all these\\napproaches attempt to modify Equation (6) based on the decomposition of Equation (3) under the self-attention settings\\nin Equation (2), which was originally proposed in Vaswani et al. [2017]. They commonly introduced to directly add\\nthe position information to the context representations. Unlikely, our approach aims to derive the relative position\\nencoding from Equation (1) under some constraints. Next, we show that the derived approach is more interpretable by\\nincorporating relative position information with the rotation of context representations.\\n3Proposed approach\\nIn this section, we discuss the proposed rotary position embedding (RoPE). We first formulate the relative position\\nencoding problem in Section (3.1), we then derive the RoPE in Section (3.2) and investigate its properties in Section (3.3).\",\n",
       "  'content_ltks': '23 rel posit embed the author of shaw et al 2018 appli differ set of equat 1a follow where p p e rd are trainabl rel posit embed note that r clip m n r min max repres the rel distanc between posit m and n they clip the rel distanc with the hypothesi that precis rel posit inform is not use beyond a certain distanc keep the form of equat 3 the author dai et al 2019 have propos to decompos qmkn of equat 2 a the key idea is to replac the absolut posit embed pn with it sinusoid encod rel counterpart pm n while the absolut posit pm in the third and fourth term with two trainabl vector u and v independ of the queri posit further w is distinguish for the content base and locat base key vector n and pn denot a wk and w k result in it is noteworthi that the posit inform in the valu term is remov by set f w later work raffel et al 2020 he et al 2020 ke et al 2020 huang et al 2020 follow these set by onli encod the rel posit inform into the attent weight howev the author of raffel et al 2020 reform equat 6 a where b j is a trainabl bia the author of ke et al 2020 investig the middl two term of equat 6 and found littl correl between absolut posit and word the author of raffel et al 2020 propos to model a pair of word or posit use differ project matrix the author of he et al 2020 argu that the rel posit of two token could onli be fulli model use the middl two term of equat 6 a a consequ the absolut posit embed pm and pn were simpli replac with the rel posit embed pm n a comparison of the four variant of the rel posit embed radford and narasimhan 2018 ha shown that the variant similar to equat 1o is the most effici among the other three gener speak all these approach attempt to modifi equat 6 base on the decomposit of equat 3 under the self attent set in equat 2 which wa origin propos in vaswani et al 2017 they commonli introduc to directli add the posit inform to the context represent unlik our approach aim to deriv the rel posit encod from equat 1 under some constraint next we show that the deriv approach is more interpret by incorpor rel posit inform with the rotat of context represent 3propos approach in thi section we discu the propos rotari posit embed rope we first formul the rel posit encod problem in section 31 we then deriv the rope in section 32 and investig it properti in section 33',\n",
       "  'content_sm_ltks': '23 rel posit embed the author of shaw et al 2018 appli differ set of equat 1a follow where p p e rd are trainabl rel posit embed note that r clip m n r min max repres the rel distanc between posit m and n they clip the rel distanc with the hypothesi that precis rel posit inform is not use beyond a certain distanc keep the form of equat 3 the author dai et al 2019 have propos to decompos qmkn of equat 2 a the key idea is to replac the absolut posit embed pn with it sinusoid encod rel counterpart pm n while the absolut posit pm in the third and fourth term with two trainabl vector u and v independ of the queri posit further w is distinguish for the content base and locat base key vector n and pn denot a wk and w k result in it is noteworthi that the posit inform in the valu term is remov by set f w later work raffel et al 2020 he et al 2020 ke et al 2020 huang et al 2020 follow these set by onli encod the rel posit inform into the attent weight howev the author of raffel et al 2020 reform equat 6 a where b j is a trainabl bia the author of ke et al 2020 investig the middl two term of equat 6 and found littl correl between absolut posit and word the author of raffel et al 2020 propos to model a pair of word or posit use differ project matrix the author of he et al 2020 argu that the rel posit of two token could onli be fulli model use the middl two term of equat 6 a a consequ the absolut posit embed pm and pn were simpli replac with the rel posit embed pm n a comparison of the four variant of the rel posit embed radford and narasimhan 2018 ha shown that the variant similar to equat 1o is the most effici among the other three gener speak all these approach attempt to modifi equat 6 base on the decomposit of equat 3 under the self attent set in equat 2 which wa origin propos in vaswani et al 2017 they commonli introduc to directli add the posit inform to the context represent unlik our approach aim to deriv the rel posit encod from equat 1 under some constraint next we show that the deriv approach is more interpret by incorpor rel posit inform with the rotat of context represent 3propos approach in thi section we discu the propos rotari posit embed rope we first formul the rel posit encod problem in section 31 we then deriv the rope in section 32 and investig it properti in section 33'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1416x1098>,\n",
       "  'page_num_int': [4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "  'position_int': [(4, 70, 542, 295, 307),\n",
       "   (4, 70, 542, 315, 329),\n",
       "   (4, 70, 542, 327, 341),\n",
       "   (4, 70, 542, 337, 351),\n",
       "   (4, 70, 542, 349, 363),\n",
       "   (4, 71, 543, 361, 373),\n",
       "   (4, 70, 542, 370, 382),\n",
       "   (4, 70, 542, 419, 433),\n",
       "   (4, 70, 542, 432, 443)],\n",
       "  'top_int': [295, 315, 327, 337, 349, 361, 370, 419, 432],\n",
       "  'content_with_weight': '3.1 Formulation\\nTransformer-based language modeling usually leverages the position information of individual tokens through a self-\\nattention mechanism. As can be observed in Equation (2), qm kn typically enables knowledge conveyance between\\ntokens at different positions. In order to incorporate relative position information, we require the inner product of query\\nQm and key kn to be formulated by a function g, which takes only the word embeddings m, Cn, and their relative\\nposition m - n as input variables. In other words, we hope that the inner product encodes position information only in\\nthe relative form:\\nThe ultimate goal is to find an equivalent encoding mechanism to solve the functions fq(m, m) and f&(n, n) to\\nconform the aforementioned relation.',\n",
       "  'content_ltks': '31 formul transform base languag model usual leverag the posit inform of individu token through a self attent mechan a can be observ in equat 2 qm kn typic enabl knowledg convey between token at differ posit in order to incorpor rel posit inform we requir the inner product of queri qm and key kn to be formul by a function g which take onli the word embed m cn and their rel posit m n a input variabl in other word we hope that the inner product encod posit inform onli in the rel form the ultim goal is to find an equival encod mechan to solv the function fq m m and f n n to conform the aforement relat',\n",
       "  'content_sm_ltks': '31 formul transform base languag model usual leverag the posit inform of individu token through a self attent mechan a can be observ in equat 2 qm kn typic enabl knowledg convey between token at differ posit in order to incorpor rel posit inform we requir the inner product of queri qm and key kn to be formul by a function g which take onli the word embed m cn and their rel posit m n a input variabl in other word we hope that the inner product encod posit inform onli in the rel form the ultim goal is to find an equival encod mechan to solv the function fq m m and f n n to conform the aforement relat'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1420x1032>,\n",
       "  'page_num_int': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       "  'position_int': [(4, 69, 542, 457, 471),\n",
       "   (4, 70, 544, 480, 491),\n",
       "   (4, 70, 543, 499, 510),\n",
       "   (4, 71, 545, 510, 522),\n",
       "   (4, 70, 544, 520, 532),\n",
       "   (4, 70, 543, 601, 615),\n",
       "   (4, 69, 542, 612, 627),\n",
       "   (4, 69, 542, 674, 692),\n",
       "   (4, 70, 544, 689, 703),\n",
       "   (4, 71, 545, 702, 712),\n",
       "   (4, 70, 544, 712, 724)],\n",
       "  'top_int': [457, 480, 499, 510, 520, 601, 612, 674, 689, 702, 712],\n",
       "  'content_with_weight': '3.2 Rotary position embedding\\n3.2.1A 2D case\\nWe begin with a simple case with a dimension d = 2. Under these settings, we make use of the geometric property\\nof vectors on a 2D plane and its complex form to prove (refer Section (3.4.1) for more details) that a solution to our\\nformulation Equation (11) is:\\nwhere Re[·] is the real part of a complex number and (W n)* represents the conjugate complex number of (W :n).\\n0 E R is a preset non-zero constant. We can further write ffq,k, in a multiplication matrix:\\nwhere (2n , &2)is Em expressed in the 2D coordinates. Similarly, g can be viewed as a matrix and thus enables the\\nsolution of formulation in Section (3.1) under the 2D case. Specifically, incorporating the relative position embedding\\nis straightforward: simply rotate the affine-transformed word embedding vector by amount of angle multiples of its\\nposition index and thus interprets the intuition behind Rotary Position Embedding.',\n",
       "  'content_ltks': '32 rotari posit embed 32 1a 2d case we begin with a simpl case with a dimens d2 under these set we make use of the geometr properti of vector on a 2d plane and it complex form to prove refer section 34 1 for more detail that a solut to our formul equat 11 is where re is the real part of a complex number and w n repres the conjug complex number of w n 0 e r is a preset non zero constant we can further write ffq k in a multipl matrix where 2n 2 is em express in the 2d coordin similarli g can be view a a matrix and thu enabl the solut of formul in section 31 under the 2d case specif incorpor the rel posit embed is straightforward simpli rotat the affin transform word embed vector by amount of angl multipl of it posit index and thu interpret the intuit behind rotari posit embed',\n",
       "  'content_sm_ltks': '32 rotari posit embed 32 1a 2d case we begin with a simpl case with a dimens d2 under these set we make use of the geometr properti of vector on a 2d plane and it complex form to prove refer section 34 1 for more detail that a solut to our formul equat 11 is where re is the real part of a complex number and w n repres the conjug complex number of w n 0 e r is a preset non zero constant we can further write ffq k in a multipl matrix where 2n 2 is em express in the 2d coordin similarli g can be view a a matrix and thu enabl the solut of formul in section 31 under the 2d case specif incorpor the rel posit embed is straightforward simpli rotat the affin transform word embed vector by amount of angl multipl of it posit index and thu interpret the intuit behind rotari posit embed'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1427x1155>,\n",
       "  'page_num_int': [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "  'position_int': [(5, 70, 545, 72, 84),\n",
       "   (5, 70, 545, 91, 105),\n",
       "   (5, 69, 544, 101, 116),\n",
       "   (5, 70, 545, 151, 162),\n",
       "   (5, 68, 544, 250, 267),\n",
       "   (5, 70, 545, 264, 279),\n",
       "   (5, 69, 544, 306, 325),\n",
       "   (5, 71, 547, 322, 334),\n",
       "   (5, 71, 547, 333, 345),\n",
       "   (5, 70, 546, 349, 361),\n",
       "   (5, 69, 544, 359, 373),\n",
       "   (5, 70, 546, 372, 383),\n",
       "   (5, 70, 545, 382, 393)],\n",
       "  'top_int': [72, 91, 101, 151, 250, 264, 306, 322, 333, 349, 359, 372, 382],\n",
       "  'content_with_weight': '3.2.2General form\\nIn order to generalize our results in 2D to any c E Rd where d is even, we divide the d-dimension space into d/2\\nsub-spaces and combine them in the merit of the linearity of the inner product, turning fq,k) into:\\nwhere\\nis the rotary matrix with pre-defined parameters = {0; = 10000-2(i-1)/d, E [1, 2, ., d/2]}. A graphic illustration\\nof RoPE is shown in Figure (1). Applying our RoPE to self-attention in Equation (2), we obtain:\\nWhere R,n-m = (R,m)T R,n. Note that R is an orthogonal matrix, which ensures stability during the process of\\nencoding position information. In addition, due to the sparsity of R?, applying matrix multiplication directly as in\\nEquation (16) is not computationally efficient; we provide another realization in theoretical explanation.\\nIn contrast to the additive nature of position embedding method adopted in the previous works, i.e., Equations (3)\\nto (10), our approach is multiplicative. Moreover, RoPE naturally incorporates relative position information through\\nrotation matrix product instead of altering terms in the expanded formulation of additive position encoding when applied\\nwith self-attention.',\n",
       "  'content_ltks': '32 2gener form in order to gener our result in 2d to ani c e rd where d is even we divid the d dimens space into d2 sub space and combin them in the merit of the linear of the inner product turn fq k into where is the rotari matrix with pre defin paramet 0 10000 2i 1 d e 12 d2 a graphic illustr of rope is shown in figur 1 appli our rope to self attent in equat 2 we obtain where r n m r m t r n note that r is an orthogon matrix which ensur stabil dure the process of encod posit inform in addit due to the sparsiti of r appli matrix multipl directli a in equat 16 is not comput effici we provid anoth realiz in theoret explan in contrast to the addit natur of posit embed method adopt in the previou work i e equat 3 to 10 our approach is multipl moreov rope natur incorpor rel posit inform through rotat matrix product instead of alter term in the expand formul of addit posit encod when appli with self attent',\n",
       "  'content_sm_ltks': '32 2gener form in order to gener our result in 2d to ani c e rd where d is even we divid the d dimens space into d2 sub space and combin them in the merit of the linear of the inner product turn fq k into where is the rotari matrix with pre defin paramet 0 10000 2i 1 d e 12 d2 a graphic illustr of rope is shown in figur 1 appli our rope to self attent in equat 2 we obtain where r n m r m t r n note that r is an orthogon matrix which ensur stabil dure the process of encod posit inform in addit due to the sparsiti of r appli matrix multipl directli a in equat 16 is not comput effici we provid anoth realiz in theoret explan in contrast to the addit natur of posit embed method adopt in the previou work i e equat 3 to 10 our approach is multipl moreov rope natur incorpor rel posit inform through rotat matrix product instead of alter term in the expand formul of addit posit encod when appli with self attent'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1426x1579>,\n",
       "  'page_num_int': [5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "  'position_int': [(5, 70, 545, 632, 644),\n",
       "   (5, 68, 543, 649, 668),\n",
       "   (5, 69, 544, 664, 678),\n",
       "   (5, 70, 546, 676, 688),\n",
       "   (5, 70, 546, 687, 698),\n",
       "   (5, 71, 547, 712, 723),\n",
       "   (6, 70, 545, 115, 129),\n",
       "   (6, 70, 545, 127, 141),\n",
       "   (6, 71, 547, 139, 151),\n",
       "   (6, 70, 546, 196, 208),\n",
       "   (6, 70, 545, 205, 219),\n",
       "   (6, 70, 546, 218, 230),\n",
       "   (6, 70, 546, 228, 242),\n",
       "   (6, 70, 546, 239, 253),\n",
       "   (6, 71, 547, 251, 263),\n",
       "   (6, 72, 547, 262, 274),\n",
       "   (6, 71, 547, 273, 284),\n",
       "   (6, 70, 546, 336, 347),\n",
       "   (6, 71, 547, 347, 359),\n",
       "   (6, 70, 545, 357, 371)],\n",
       "  'top_int': [632,\n",
       "   649,\n",
       "   664,\n",
       "   676,\n",
       "   687,\n",
       "   712,\n",
       "   115,\n",
       "   127,\n",
       "   139,\n",
       "   196,\n",
       "   205,\n",
       "   218,\n",
       "   228,\n",
       "   239,\n",
       "   251,\n",
       "   262,\n",
       "   273,\n",
       "   336,\n",
       "   347,\n",
       "   357],\n",
       "  'content_with_weight': '3.3Properties of RoPE\\nLong-term decay: Following Vaswani et al [2017], we set 0; = 10000-2a/d. One can prove that this seting provides\\na long-term decay property (refer to Section (3.4.3) for more details), which means the inner-product will decay when\\nthe relative position increase. This property coincides with the intuition that a pair of tokens with a long relative distance\\nshould have less connection.\\nRoPE with linear attention: The self-attention can be rewritten in a more general form.\\nThe original self-attention chooses sim(qm, kn) = exp(qmkn / √d). Note that the original self-attention should\\ncompute the inner product of query and key for every pair of tokens, which has a quadratic complexity O(N2). Follow\\nKatharopoulos et al. [2020], the linear attentions reformulate Equation (17) as\\nwhere Φ(), Φ(） are usually non-negative functions. The authors of Katharopoulos et al. [2020] have proposed\\nΦ(x) = Φ(x) = elu(c) + 1 and first computed the multiplication between keys and values using the associative property\\nof matrix multiplication. A softmax function is used in Shen et al. [2021] to normalize queries and keys separately\\nbefore the inner product, which is equivalent to Φ(qi) = softmax(q;) and Φ(k) = exp(k). For more details about\\nlinear attention, we encourage readers to refer to original papers. In this section, we focus on discussing incorporating\\nRoPE with Equation (18). Since RoPE injects position information by rotation, which keeps the norm of hidden\\nrepresentations unchanged, we can combine RoPE with linear attention by multiplying the rotation matrix with the\\noutputs of the non-negative functions.\\nIt is noteworthy that we keep the denominator unchanged to avoid the risk of dividing zero, and the summation in\\nthe numerator could contain negative terms. Although the weights for each value v in Equation (19) are not strictly\\nprobabilistic normalized, we kindly argue that the computation can still model the importance of values.',\n",
       "  'content_ltks': '3 3properti of rope long term decay follow vaswani et al 2017 we set 0 10000 2a d one can prove that thi sete provid a long term decay properti refer to section 34 3 for more detail which mean the inner product will decay when the rel posit increas thi properti coincid with the intuit that a pair of token with a long rel distanc should have less connect rope with linear attent the self attent can be rewritten in a more gener form the origin self attent choos sim qm kn exp qmkn d note that the origin self attent should comput the inner product of queri and key for everi pair of token which ha a quadrat complex o n2 follow katharopoulo et al 2020 the linear attent reformul equat 17 a where φ φ are usual non neg function the author of katharopoulo et al 2020 have propos φ x φ x elu c1 and first comput the multipl between key and valu use the associ properti of matrix multipl a softmax function is use in shen et al 2021 to normal queri and key separ befor the inner product which is equival to φ qi softmax q and φ k exp k for more detail about linear attent we encourag reader to refer to origin paper in thi section we focu on discuss incorpor rope with equat 18 sinc rope inject posit inform by rotat which keep the norm of hidden represent unchang we can combin rope with linear attent by multipli the rotat matrix with the output of the non neg function it is noteworthi that we keep the denomin unchang to avoid the risk of divid zero and the summat in the numer could contain neg term although the weight for each valu v in equat 19 are not strictli probabilist normal we kindli argu that the comput can still model the import of valu',\n",
       "  'content_sm_ltks': '3 3properti of rope long term decay follow vaswani et al 2017 we set 0 10000 2a d one can prove that thi sete provid a long term decay properti refer to section 34 3 for more detail which mean the inner product will decay when the rel posit increas thi properti coincid with the intuit that a pair of token with a long rel distanc should have less connect rope with linear attent the self attent can be rewritten in a more gener form the origin self attent choos sim qm kn exp qmkn d note that the origin self attent should comput the inner product of queri and key for everi pair of token which ha a quadrat complex o n2 follow katharopoulo et al 2020 the linear attent reformul equat 17 a where φ φ are usual non neg function the author of katharopoulo et al 2020 have propos φ x φ x elu c1 and first comput the multipl between key and valu use the associ properti of matrix multipl a softmax function is use in shen et al 2021 to normal queri and key separ befor the inner product which is equival to φ qi softmax q and φ k exp k for more detail about linear attent we encourag reader to refer to origin paper in thi section we focu on discuss incorpor rope with equat 18 sinc rope inject posit inform by rotat which keep the norm of hidden represent unchang we can combin rope with linear attent by multipli the rotat matrix with the output of the non neg function it is noteworthi that we keep the denomin unchang to avoid the risk of divid zero and the summat in the numer could contain neg term although the weight for each valu v in equat 19 are not strictli probabilist normal we kindli argu that the comput can still model the import of valu'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1417x1851>,\n",
       "  'page_num_int': [6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   6,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7,\n",
       "   7],\n",
       "  'position_int': [(6, 70, 542, 381, 392),\n",
       "   (6, 70, 543, 401, 413),\n",
       "   (6, 70, 543, 420, 434),\n",
       "   (6, 70, 543, 432, 444),\n",
       "   (6, 70, 542, 477, 491),\n",
       "   (6, 68, 540, 486, 503),\n",
       "   (6, 70, 543, 525, 536),\n",
       "   (6, 69, 541, 571, 585),\n",
       "   (6, 70, 542, 582, 596),\n",
       "   (6, 71, 544, 595, 607),\n",
       "   (6, 70, 542, 662, 676),\n",
       "   (6, 71, 544, 675, 687),\n",
       "   (7, 70, 542, 72, 86),\n",
       "   (7, 71, 544, 128, 140),\n",
       "   (7, 70, 543, 145, 156),\n",
       "   (7, 71, 544, 194, 206),\n",
       "   (7, 71, 544, 260, 271),\n",
       "   (7, 70, 542, 269, 283),\n",
       "   (7, 70, 542, 281, 295),\n",
       "   (7, 68, 540, 291, 307),\n",
       "   (7, 70, 542, 330, 344),\n",
       "   (7, 70, 543, 367, 378),\n",
       "   (7, 70, 543, 403, 414),\n",
       "   (7, 70, 543, 467, 481),\n",
       "   (7, 70, 543, 479, 491),\n",
       "   (7, 70, 543, 528, 540)],\n",
       "  'top_int': [381,\n",
       "   401,\n",
       "   420,\n",
       "   432,\n",
       "   477,\n",
       "   486,\n",
       "   525,\n",
       "   571,\n",
       "   582,\n",
       "   595,\n",
       "   662,\n",
       "   675,\n",
       "   72,\n",
       "   128,\n",
       "   145,\n",
       "   194,\n",
       "   260,\n",
       "   269,\n",
       "   281,\n",
       "   291,\n",
       "   330,\n",
       "   367,\n",
       "   403,\n",
       "   467,\n",
       "   479,\n",
       "   528],\n",
       "  'content_with_weight': '3.4Theoretical Explanation\\n3.4.1 Derivation of RoPE under 2D\\nUnder the case of d = 2, we consider two-word embedding vectors 3q, ck corresponds to query and key and their\\nposition m and n, respectively. According to eq. (1), their position-encoded counterparts are:\\nwhere the subscripts of qm and kn indicate the encoded positions information. Assume that there exists a function g\\nthat defines the inner product between vectors produced by fq,k:\\nwe further require below initial condition to be satisfied:\\nwhich can be read as the vectors with empty position information encoded. Given these settings, we attempt to find a\\nsolution of fq, f. First, we take advantage of the geometric meaning of vector in 2D and its complex counter part,\\ndecompose functions in Equations (20) and (21) into:\\nwhere Rf, Rg and Of, Og are the radical and angular components for f(q,b} and g, respectively. Plug them into\\nEquation (21), we get the relation:\\nwith the corresponding initial condition as:\\nwhere llqll, Ikll and Oq, Ok are the radial and angular part of q and k on the 2D plane.\\nNext, we set m = n in Equation (24) and take into account initial conditions in Equation (25):\\nOn one hand, from, a straightforward solution of Rf could be formed from Equation (26a) :\\nwhich interprets the radial functions Rq, Rk and Rg are independent from the position information. On the other hand,\\nas can be noticed in Equation (26b), Oa(cq, m) - 0q = Ok(ck, m) - 0k indicates that the angular functions does not\\ndependent on query and key, we set them to Of := Oq = Ok and term Of(α{q,k}, m) - 0{q,k} is a function of position\\nm and is independent of word embedding q,k}, we denote it as Φ(m), yielding:\\nFurther, by plugging n = m + 1 to Equation (24) and consider the above equation, we can get:\\nSince RHS is a constant irrelevant to m, Φ(m) with continuous integer inputs produce an arithmetic progression:\\nwhere 0, E R are constants and θ is non-zero. To summarize our solutions from Equations (27) to (30):\\nNote that we do not apply any constrains to fq and fk of Equation (22), thus fa(αm, 0) and fk(n, 0) are left to choose\\nfreely. To make our results comparable to Equation (3), we define:\\nThen, we simply set = 0 in Equation (31) of the final solution:',\n",
       "  'content_ltks': '3 4theoret explan 34 1 deriv of rope under 2d under the case of d2 we consid two word embed vector 3q ck correspond to queri and key and their posit m and n respect accord to eq 1 their posit encod counterpart are where the subscript of qm and kn indic the encod posit inform assum that there exist a function g that defin the inner product between vector produc by fq k we further requir below initi condit to be satisfi which can be read a the vector with empti posit inform encod given these set we attempt to find a solut of fq f first we take advantag of the geometr mean of vector in 2d and it complex counter part decompos function in equat 20 and 21 into where rf rg and of og are the radic and angular compon for f q b and g respect plug them into equat 21 we get the relat with the correspond initi condit a where llqll ikll and oq ok are the radial and angular part of q and k on the 2d plane next we set m n in equat 24 and take into account initi condit in equat 25 on one hand from a straightforward solut of rf could be form from equat 26a which interpret the radial function rq rk and rg are independ from the posit inform on the other hand a can be notic in equat 26b oa cq m 0q ok ck m 0k indic that the angular function doe not depend on queri and key we set them to of oq ok and term of α q k m 0 q k is a function of posit m and is independ of word embed q k we denot it a φ m yield further by plug n m1 to equat 24 and consid the abov equat we can get sinc rh is a constant irrelev to m φ m with continu integ input produc an arithmet progress where 0 e r are constant and θ is non zero to summar our solut from equat 27 to 30 note that we do not appli ani constrain to fq and fk of equat 22 thu fa α m 0 and fk n 0 are left to choos freeli to make our result compar to equat 3 we defin then we simpli set 0 in equat 31 of the final solut',\n",
       "  'content_sm_ltks': '3 4theoret explan 34 1 deriv of rope under 2d under the case of d2 we consid two word embed vector 3q ck correspond to queri and key and their posit m and n respect accord to eq 1 their posit encod counterpart are where the subscript of qm and kn indic the encod posit inform assum that there exist a function g that defin the inner product between vector produc by fq k we further requir below initi condit to be satisfi which can be read a the vector with empti posit inform encod given these set we attempt to find a solut of fq f first we take advantag of the geometr mean of vector in 2d and it complex counter part decompos function in equat 20 and 21 into where rf rg and of og are the radic and angular compon for f q b and g respect plug them into equat 21 we get the relat with the correspond initi condit a where llqll ikll and oq ok are the radial and angular part of q and k on the 2d plane next we set m n in equat 24 and take into account initi condit in equat 25 on one hand from a straightforward solut of rf could be form from equat 26a which interpret the radial function rq rk and rg are independ from the posit inform on the other hand a can be notic in equat 26b oa cq m 0q ok ck m 0k indic that the angular function doe not depend on queri and key we set them to of oq ok and term of α q k m 0 q k is a function of posit m and is independ of word embed q k we denot it a φ m yield further by plug n m1 to equat 24 and consid the abov equat we can get sinc rh is a constant irrelev to m φ m with continu integ input produc an arithmet progress where 0 e r are constant and θ is non zero to summar our solut from equat 27 to 30 note that we do not appli ani constrain to fq and fk of equat 22 thu fa α m 0 and fk n 0 are left to choos freeli to make our result compar to equat 3 we defin then we simpli set 0 in equat 31 of the final solut'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1419x834>,\n",
       "  'page_num_int': [7, 7, 7],\n",
       "  'position_int': [(7, 70, 543, 587, 599),\n",
       "   (7, 70, 543, 606, 623),\n",
       "   (7, 71, 544, 620, 632)],\n",
       "  'top_int': [587, 606, 620],\n",
       "  'content_with_weight': '3.4.2Computational efficient realization of rotary matrix multiplication\\nTaking the advantage of the sparsity of R,m in Equation (15), a more computational efcient realization of a\\nmultiplication of Rd and E Rd is:',\n",
       "  'content_ltks': '34 2comput effici realiz of rotari matrix multipl take the advantag of the sparsiti of r m in equat 15 a more comput efcient realiz of a multipl of rd and e rd is',\n",
       "  'content_sm_ltks': '34 2comput effici realiz of rotari matrix multipl take the advantag of the sparsiti of r m in equat 15 a more comput efcient realiz of a multipl of rd and e rd is'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1412x1017>,\n",
       "  'page_num_int': [8, 8, 8, 8, 8, 8, 8],\n",
       "  'position_int': [(8, 70, 541, 283, 295),\n",
       "   (8, 70, 541, 303, 315),\n",
       "   (8, 70, 540, 314, 325),\n",
       "   (8, 68, 538, 376, 395),\n",
       "   (8, 90, 561, 390, 406),\n",
       "   (8, 69, 540, 461, 473),\n",
       "   (8, 70, 540, 615, 627)],\n",
       "  'top_int': [283, 303, 314, 376, 390, 461, 615],\n",
       "  'content_with_weight': '3.4.3 Long-term decay of RoPE\\nWe can group entries of vectors q = W qcm and k = W n in pairs, and the inner product of RoPE in Equation (16)\\ncan be written as a complex number multiplication.\\nwhere q[2i:2i+1] represents the 2ith to (2i + 1)th entries of q. Denote hi = Q[(2:2i+1]b(2i:2i+1l and Sj=\\ne(m-n)θ, and let ha/2 = 0 and So = 0, we can rewrite the summation using Abel transformation\\nThus,\\nshown in Figure (2).',\n",
       "  'content_ltks': '34 3 long term decay of rope we can group entri of vector q w qcm and k w n in pair and the inner product of rope in equat 16 can be written a a complex number multipl where q 2i 2i 1 repres the 2ith to 2i 1 th entri of q denot hi q 2 2i 1 b 2i 2i 1l and sj e m n θ and let ha 20 and so 0 we can rewrit the summat use abel transform thu shown in figur 2',\n",
       "  'content_sm_ltks': '34 3 long term decay of rope we can group entri of vector q w qcm and k w n in pair and the inner product of rope in equat 16 can be written a a complex number multipl where q 2i 2i 1 repres the 2ith to 2i 1 th entri of q denot hi q 2 2i 1 b 2i 2i 1l and sj e m n θ and let ha 20 and so 0 we can rewrit the summat use abel transform thu shown in figur 2'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1419x1122>,\n",
       "  'page_num_int': [8, 8, 8, 8, 8, 8, 9, 9, 9, 9],\n",
       "  'position_int': [(8, 70, 543, 643, 655),\n",
       "   (8, 70, 543, 668, 679),\n",
       "   (8, 70, 543, 679, 690),\n",
       "   (8, 69, 542, 688, 702),\n",
       "   (8, 70, 543, 699, 713),\n",
       "   (8, 69, 542, 712, 725),\n",
       "   (9, 70, 543, 71, 82),\n",
       "   (9, 70, 543, 82, 94),\n",
       "   (9, 70, 543, 166, 178),\n",
       "   (9, 69, 542, 178, 189)],\n",
       "  'top_int': [643, 668, 679, 688, 699, 712, 71, 82, 166, 178],\n",
       "  'content_with_weight': '4 Experiments and Evaluation\\nWe evaluate the proposed RoFormer on various NLP tasks as follows. We validate the performance of the proposed\\nsolution on machine translation task Section (4.1). Then, we compare our RoPE implementation with BERTDevlin\\net al. [2019] during the pre-training stage in Section (4.2). Based on the pre-trained model, in Section (4.3), we further\\ncarry out evaluations across different downstream tasks from GLUE benchmarksSingh et al. [2018]. In Addition, we\\nconduct experiments using the proposed RoPE with the linear attention of PerFormer Choromanski et al. [2020] in\\nTable 1: The proposed RoFormer gives better BLEU scores compared to its baseline alternative Vaswani et al. [2017]\\non the WMT 2014 English-to-German translation taskBojar et al. [2014].\\nSection (4.4). By the end, additional tests on Chinese data are included in Section (4.5). All the experiments were run\\non two cloud severs with 4 x V100 GPUs.',\n",
       "  'content_ltks': '4 experi and evalu we evalu the propos roform on variou nlp task a follow we valid the perform of the propos solut on machin translat task section 4 1 then we compar our rope implement with bertdevlin et al 2019 dure the pre train stage in section 42 base on the pre train model in section 43 we further carri out evalu across differ downstream task from glue benchmarkssingh et al 2018 in addit we conduct experi use the propos rope with the linear attent of perform choromanski et al 2020 in tabl 1 the propos roform give better bleu score compar to it baselin altern vaswani et al 2017 on the wmt 2014 english to german translat taskbojar et al 2014 section 44 by the end addit test on chines data are includ in section 45 all the experi were run on two cloud sever with 4 x v100 gpu',\n",
       "  'content_sm_ltks': '4 experi and evalu we evalu the propos roform on variou nlp task a follow we valid the perform of the propos solut on machin translat task section 4 1 then we compar our rope implement with bertdevlin et al 2019 dure the pre train stage in section 42 base on the pre train model in section 43 we further carri out evalu across differ downstream task from glue benchmarkssingh et al 2018 in addit we conduct experi use the propos rope with the linear attent of perform choromanski et al 2020 in tabl 1 the propos roform give better bleu score compar to it baselin altern vaswani et al 2017 on the wmt 2014 english to german translat taskbojar et al 2014 section 44 by the end addit test on chines data are includ in section 45 all the experi were run on two cloud sever with 4 x v100 gpu'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1247x783>,\n",
       "  'page_num_int': [9, 9],\n",
       "  'position_int': [(9, 70, 486, 203, 214), (9, 70, 485, 222, 236)],\n",
       "  'top_int': [203, 222],\n",
       "  'content_with_weight': '4.1Machine Translation\\nWe first demonstrate the performance of RoFormer on sequence-to-sequence language translation tasks.',\n",
       "  'content_ltks': '4 1machin translat we first demonstr the perform of roform on sequenc to sequenc languag translat task',\n",
       "  'content_sm_ltks': '4 1machin translat we first demonstr the perform of roform on sequenc to sequenc languag translat task'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1406x819>,\n",
       "  'page_num_int': [9, 9, 9],\n",
       "  'position_int': [(9, 70, 539, 246, 258),\n",
       "   (9, 71, 540, 266, 278),\n",
       "   (9, 71, 540, 278, 289)],\n",
       "  'top_int': [246, 266, 278],\n",
       "  'content_with_weight': '4.1.1 Experimental Settings\\nWe choose the standard WMT 2014 English-German datasetBojar et al. [2014], which consists of approximately 4.5\\nmillion sentence pairs. We compare to the transformer-based baseline alternative Vaswani et al. [2017].',\n",
       "  'content_ltks': '4 11 experiment set we choos the standard wmt 2014 english german datasetbojar et al 2014 which consist of approxim 45 million sentenc pair we compar to the transform base baselin altern vaswani et al 2017',\n",
       "  'content_sm_ltks': '4 11 experiment set we choos the standard wmt 2014 english german datasetbojar et al 2014 which consist of approxim 45 million sentenc pair we compar to the transform base baselin altern vaswani et al 2017'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1417x1075>,\n",
       "  'page_num_int': [9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       "  'position_int': [(9, 70, 543, 300, 311),\n",
       "   (9, 69, 541, 318, 332),\n",
       "   (9, 70, 543, 331, 343),\n",
       "   (9, 71, 544, 343, 354),\n",
       "   (9, 70, 543, 353, 364),\n",
       "   (9, 71, 544, 364, 376),\n",
       "   (9, 71, 544, 375, 387),\n",
       "   (9, 71, 544, 386, 397),\n",
       "   (9, 69, 541, 396, 410)],\n",
       "  'top_int': [300, 318, 331, 343, 353, 364, 375, 386, 396],\n",
       "  'content_with_weight': '4.1.2 Implementation details\\nWe carry out some modifications on self-attention layer of the baseline model Vaswani et al. [2017] to enable RoPE to\\nits learning process. We replicate the setup for English-to-German translation with a vocabulary of 37k based on a joint\\nsource and target byte pair encoding(BPE)Sennrich et al. [2015]. During the evaluation, a single model is obtained\\nby averaging the last 5 checkpoints. The result uses beam search with a beam size of 4 and length penalty 0.6. We\\nimplement the experiment in PyTorch in the fairseq toolkit (MIT License)Ott et al. [2019]. Our model is optimized\\nwith the Adam optimizer using β1 = 0.9, β2 = 0.98, learning rate is increased linearly from 1e - 7 to 5e - 4 and then\\ndecayed proportionally to the inverse square root of the step number. Label smoothing with 0.1 is also adopted. We\\nreport the BLEUPapineni et al. [2002] score on the test set as the final metric.',\n",
       "  'content_ltks': '4 12 implement detail we carri out some modif on self attent layer of the baselin model vaswani et al 2017 to enabl rope to it learn process we replic the setup for english to german translat with a vocabulari of 37k base on a joint sourc and target byte pair encod bpe sennrich et al 2015 dure the evalu a singl model is obtain by averag the last 5 checkpoint the result us beam search with a beam size of 4 and length penalti 0 6 we implement the experi in pytorch in the fairseq toolkit mit licens ott et al 2019 our model is optim with the adam optim use β 10 9 β 20 98 learn rate is increas linearli from 1e 7 to 5e 4 and then decay proport to the invers squar root of the step number label smooth with 01 is also adopt we report the bleupapineni et al 2002 score on the test set a the final metric',\n",
       "  'content_sm_ltks': '4 12 implement detail we carri out some modif on self attent layer of the baselin model vaswani et al 2017 to enabl rope to it learn process we replic the setup for english to german translat with a vocabulari of 37k base on a joint sourc and target byte pair encod bpe sennrich et al 2015 dure the evalu a singl model is obtain by averag the last 5 checkpoint the result us beam search with a beam size of 4 and length penalti 0 6 we implement the experi in pytorch in the fairseq toolkit mit licens ott et al 2019 our model is optim with the adam optim use β 10 9 β 20 98 learn rate is increas linearli from 1e 7 to 5e 4 and then decay proport to the invers squar root of the step number label smooth with 01 is also adopt we report the bleupapineni et al 2002 score on the test set a the final metric'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1409x810>,\n",
       "  'page_num_int': [9, 9, 9],\n",
       "  'position_int': [(9, 72, 541, 421, 430),\n",
       "   (9, 70, 540, 439, 451),\n",
       "   (9, 71, 541, 451, 462)],\n",
       "  'top_int': [421, 439, 451],\n",
       "  'content_with_weight': '4.1.3Results\\nWe train the baseline model and our RoFormer under the same settings and report the results in Table (1). As can be\\nseen, our model gives better BLEU scores compared to the baseline Transformer.',\n",
       "  'content_ltks': '4 1 3result we train the baselin model and our roform under the same set and report the result in tabl 1a can be seen our model give better bleu score compar to the baselin transform',\n",
       "  'content_sm_ltks': '4 1 3result we train the baselin model and our roform under the same set and report the result in tabl 1a can be seen our model give better bleu score compar to the baselin transform'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1414x819>,\n",
       "  'page_num_int': [9, 9, 9],\n",
       "  'position_int': [(9, 70, 541, 475, 486),\n",
       "   (9, 70, 542, 496, 508),\n",
       "   (9, 70, 541, 507, 519)],\n",
       "  'top_int': [475, 496, 507],\n",
       "  'content_with_weight': '4.2Pre-training Language Modeling\\nThe second experiment is to validate the performance of our proposal in terms of learning contextual representations. To\\nachieve this, we replace the original sinusoidal position encoding of BERT with our RoPE during the pre-training step.',\n",
       "  'content_ltks': '4 2pre train languag model the second experi is to valid the perform of our propos in term of learn contextu represent to achiev thi we replac the origin sinusoid posit encod of bert with our rope dure the pre train step',\n",
       "  'content_sm_ltks': '4 2pre train languag model the second experi is to valid the perform of our propos in term of learn contextu represent to achiev thi we replac the origin sinusoid posit encod of bert with our rope dure the pre train step'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1406x904>,\n",
       "  'page_num_int': [9, 9, 9, 9, 9],\n",
       "  'position_int': [(9, 70, 538, 528, 542),\n",
       "   (9, 71, 540, 550, 561),\n",
       "   (9, 71, 540, 561, 572),\n",
       "   (9, 71, 540, 571, 583),\n",
       "   (9, 71, 540, 583, 594)],\n",
       "  'top_int': [528, 550, 561, 571, 583],\n",
       "  'content_with_weight': '4.2.1 Experimental Settings\\nWe use the BookCorpus Zhu et al. [2015] and the Wikipedia Corpus Foundation [2021] from Huggingface Datasets\\nlibrary (Apache License 2.0) for pre-training. The corpus is further split into train and validation sets at 8:2 ratio. We\\nuse the masked language-modeling (MLM) loss values of the training process as an evaluation metric. The well-known\\nBERT Devlin et al. [2019] is adopted as our baseline model. Note that we use bert-base-uncased in our experiments.',\n",
       "  'content_ltks': '42 1 experiment set we use the bookcorpu zhu et al 2015 and the wikipedia corpu foundat 2021 from huggingfac dataset librari apach licens 20 for pre train the corpu is further split into train and valid set at 8 2 ratio we use the mask languag model mlm loss valu of the train process a an evalu metric the well known bert devlin et al 2019 is adopt a our baselin model note that we use bert base uncas in our experi',\n",
       "  'content_sm_ltks': '42 1 experiment set we use the bookcorpu zhu et al 2015 and the wikipedia corpu foundat 2021 from huggingfac dataset librari apach licens 20 for pre train the corpu is further split into train and valid set at 8 2 ratio we use the mask languag model mlm loss valu of the train process a an evalu metric the well known bert devlin et al 2019 is adopt a our baselin model note that we use bert base uncas in our experi'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1415x908>,\n",
       "  'page_num_int': [9, 9, 9, 9, 9],\n",
       "  'position_int': [(9, 71, 543, 607, 618),\n",
       "   (9, 71, 543, 626, 637),\n",
       "   (9, 69, 541, 636, 650),\n",
       "   (9, 70, 542, 646, 660),\n",
       "   (9, 71, 543, 659, 670)],\n",
       "  'top_int': [607, 626, 636, 646, 659],\n",
       "  'content_with_weight': '4.2.2Implementation details\\nFor RoFormer, we replace the sinusoidal position encoding in the self-attention block of the baseline model with our\\nproposed RoPE and realizes self-attention according to Equation (16). We train both BERT and RoFormer with batch\\nsize 64 and maximum sequence length of 512 for 100k steps. AdamW Loshchilov and Hutter [2017] is used as the\\noptimizer with learning rate 1e-5.',\n",
       "  'content_ltks': '42 2implement detail for roform we replac the sinusoid posit encod in the self attent block of the baselin model with our propos rope and realiz self attent accord to equat 16 we train both bert and roform with batch size 64 and maximum sequenc length of 512 for 100k step adamw loshchilov and hutter 2017 is use a the optim with learn rate 1e 5',\n",
       "  'content_sm_ltks': '42 2implement detail for roform we replac the sinusoid posit encod in the self attent block of the baselin model with our propos rope and realiz self attent accord to equat 16 we train both bert and roform with batch size 64 and maximum sequenc length of 512 for 100k step adamw loshchilov and hutter 2017 is use a the optim with learn rate 1e 5'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1417x669>,\n",
       "  'page_num_int': [9, 9, 9],\n",
       "  'position_int': [(9, 70, 543, 681, 693),\n",
       "   (9, 69, 541, 699, 713),\n",
       "   (9, 70, 543, 712, 724)],\n",
       "  'top_int': [681, 699, 712],\n",
       "  'content_with_weight': '4.2.3Results\\nThe MLM loss during pre-training is shown on the left plot of Figure (3). Compare to the vanilla BERT, RoFormer\\nexperiences faster convergence.',\n",
       "  'content_ltks': '42 3result the mlm loss dure pre train is shown on the left plot of figur 3 compar to the vanilla bert roform experi faster converg',\n",
       "  'content_sm_ltks': '42 3result the mlm loss dure pre train is shown on the left plot of figur 3 compar to the vanilla bert roform experi faster converg'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1409x817>,\n",
       "  'page_num_int': [10, 10, 10],\n",
       "  'position_int': [(10, 70, 540, 279, 290),\n",
       "   (10, 71, 541, 300, 312),\n",
       "   (10, 70, 540, 312, 323)],\n",
       "  'top_int': [279, 300, 312],\n",
       "  'content_with_weight': '4.3Fine-tuning on GLUE tasks\\nConsistent with the previous experiments, we fine-tune the weights of our pre-trained RoFormer across various GLUE\\ntasks in order to evaluate its generalization ability on the downstream NLP tasks.',\n",
       "  'content_ltks': '4 3fine tune on glue task consist with the previou experi we finetune the weight of our pre train roform across variou glue task in order to evalu it gener abil on the downstream nlp task',\n",
       "  'content_sm_ltks': '4 3fine tune on glue task consist with the previou experi we finetune the weight of our pre train roform across variou glue task in order to evalu it gener abil on the downstream nlp task'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1419x906>,\n",
       "  'page_num_int': [10, 10, 10, 10, 10],\n",
       "  'position_int': [(10, 70, 543, 340, 351),\n",
       "   (10, 70, 543, 359, 371),\n",
       "   (10, 70, 543, 371, 382),\n",
       "   (10, 69, 542, 380, 394),\n",
       "   (10, 69, 542, 391, 403)],\n",
       "  'top_int': [340, 359, 371, 380, 391],\n",
       "  'content_with_weight': '4.3.1 Experimental Settings\\nWe look at several datasets from GLUE, i.e. MRPC Dolan and Brockett [2005], SST-2 Socher et al. [2013], QNLI\\nRajpurkar et al. [2016], STS-B Al-Natsheh [2017], QQP Chen et al. [2018b] and MNLI Williams et al. [2018]. We use\\nF1-score for MRPC and QQP dataset, spearman correlation for STS-B, and accuracy for the remaining as the evaluation\\nmetrics.',\n",
       "  'content_ltks': '43 1 experiment set we look at sever dataset from glue i e mrpc dolan and brockett 2005 sst 2 socher et al 2013 qnli rajpurkar et al 2016 st b al natsheh 2017 qqp chen et al 2018b and mnli william et al 2018 we use f1 score for mrpc and qqp dataset spearman correl for st b and accuraci for the remain a the evalu metric',\n",
       "  'content_sm_ltks': '43 1 experiment set we look at sever dataset from glue i e mrpc dolan and brockett 2005 sst 2 socher et al 2013 qnli rajpurkar et al 2016 st b al natsheh 2017 qqp chen et al 2018b and mnli william et al 2018 we use f1 score for mrpc and qqp dataset spearman correl for st b and accuraci for the remain a the evalu metric'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1412x855>,\n",
       "  'page_num_int': [10, 10, 10, 10],\n",
       "  'position_int': [(10, 71, 542, 421, 432),\n",
       "   (10, 70, 541, 442, 453),\n",
       "   (10, 71, 542, 453, 464),\n",
       "   (10, 70, 541, 463, 475)],\n",
       "  'top_int': [421, 442, 453, 463],\n",
       "  'content_with_weight': '4.3.2 Implementation details\\nWe use Huggingface Transformers library (Apache License 2.0)Wolf et al. [2020] to fine-tune each of the aforementioned\\ndownstream tasks for 3 epochs, with a maximum sequence length of 512, batch size of 32 and learning rates 2,3,4,5e-5.\\nFollowing Devlin et al. [2019], we report the best-averaged results on the validation set.',\n",
       "  'content_ltks': '43 2 implement detail we use huggingfac transform librari apach licens 20 wolf et al 2020 to finetune each of the aforement downstream task for 3 epoch with a maximum sequenc length of 512 batch size of 32 and learn rate 23 4 5e 5 follow devlin et al 2019 we report the best averag result on the valid set',\n",
       "  'content_sm_ltks': '43 2 implement detail we use huggingfac transform librari apach licens 20 wolf et al 2020 to finetune each of the aforement downstream task for 3 epoch with a maximum sequenc length of 512 batch size of 32 and learn rate 23 4 5e 5 follow devlin et al 2019 we report the best averag result on the valid set'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1404x818>,\n",
       "  'page_num_int': [10, 10, 10],\n",
       "  'position_int': [(10, 70, 538, 585, 596),\n",
       "   (10, 70, 538, 604, 616),\n",
       "   (10, 70, 538, 617, 628)],\n",
       "  'top_int': [585, 604, 617],\n",
       "  'content_with_weight': '4.3.3Results\\nThe evaluation results of the fine-tuning tasks are reported in Table (2). As can be seen, RoFormer can significantly\\noutperform BERT in three out of six datasets, and the improvements are considerable.',\n",
       "  'content_ltks': '43 3result the evalu result of the finetune task are report in tabl 2 a can be seen roform can significantli outperform bert in three out of six dataset and the improv are consider',\n",
       "  'content_sm_ltks': '43 3result the evalu result of the finetune task are report in tabl 2 a can be seen roform can significantli outperform bert in three out of six dataset and the improv are consider'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1417x796>,\n",
       "  'page_num_int': [10, 10, 10, 10, 10, 10],\n",
       "  'position_int': [(10, 70, 543, 646, 656),\n",
       "   (10, 70, 542, 667, 679),\n",
       "   (10, 70, 543, 679, 691),\n",
       "   (10, 71, 544, 690, 702),\n",
       "   (10, 69, 541, 698, 712),\n",
       "   (10, 70, 542, 710, 722)],\n",
       "  'top_int': [646, 667, 679, 690, 698, 710],\n",
       "  'content_with_weight': '4.4 Performer with RoPE\\nPerformer Choromanski et al. [2020] introduces an alternative attention mechanism, linear attention, which is designed\\nto avoid quadratic computation cost that scales with input sequence length. As discussed in Section (3.3), the proposed\\nRoPE can be easily implemented in the PerFormer model to realize the relative position encoding while keeping its\\nlinearly scaled complexity in self-attention. We demonstrate its performance with the pre-training task of language\\nmodeling.',\n",
       "  'content_ltks': '44 perform with rope perform choromanski et al 2020 introduc an altern attent mechan linear attent which is design to avoid quadrat comput cost that scale with input sequenc length a discuss in section 33 the propos rope can be easili implement in the perform model to realiz the rel posit encod while keep it linearli scale complex in self attent we demonstr it perform with the pre train task of languag model',\n",
       "  'content_sm_ltks': '44 perform with rope perform choromanski et al 2020 introduc an altern attent mechan linear attent which is design to avoid quadrat comput cost that scale with input sequenc length a discuss in section 33 the propos rope can be easili implement in the perform model to realiz the rel posit encod while keep it linearli scale complex in self attent we demonstr it perform with the pre train task of languag model'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1412x798>,\n",
       "  'page_num_int': [11, 11, 11, 11, 11, 11],\n",
       "  'position_int': [(11, 70, 541, 73, 85),\n",
       "   (11, 70, 540, 91, 103),\n",
       "   (11, 71, 542, 104, 115),\n",
       "   (11, 70, 541, 113, 124),\n",
       "   (11, 71, 542, 126, 137),\n",
       "   (11, 70, 541, 135, 146)],\n",
       "  'top_int': [73, 91, 104, 113, 126, 135],\n",
       "  'content_with_weight': '4.4.1 Implementation details\\nWe carry out tests on the Enwik8 dataset Mahoney [2006], which is from English Wikipedia that includes markup,\\nspecial characters and text in other languages in addition to English text. We incorporate RoPE into the 12 layer\\nchar-based PerFormer with 768 dimensions and 12 heads2. To better illustrate the efficacy of RoPE, we report the loss\\ncurves of the pre-training process with and without RoPE under the same settings, i.e., learning rate 1e-4, batch size\\n128 and a fixed maximum sequence length of 1024, etc.',\n",
       "  'content_ltks': '44 1 implement detail we carri out test on the enwik8 dataset mahoney 2006 which is from english wikipedia that includ markup special charact and text in other languag in addit to english text we incorpor rope into the 12 layer char base perform with 768 dimens and 12 heads2 to better illustr the efficaci of rope we report the loss curv of the pre train process with and without rope under the same set i e learn rate 1e 4 batch size 128 and a fix maximum sequenc length of 1024 etc',\n",
       "  'content_sm_ltks': '44 1 implement detail we carri out test on the enwik8 dataset mahoney 2006 which is from english wikipedia that includ markup special charact and text in other languag in addit to english text we incorpor rope into the 12 layer char base perform with 768 dimens and 12 heads2 to better illustr the efficaci of rope we report the loss curv of the pre train process with and without rope under the same set i e learn rate 1e 4 batch size 128 and a fix maximum sequenc length of 1024 etc'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1419x866>,\n",
       "  'page_num_int': [11, 11, 11, 11],\n",
       "  'position_int': [(11, 70, 543, 158, 170),\n",
       "   (11, 70, 543, 175, 189),\n",
       "   (11, 69, 542, 187, 201),\n",
       "   (11, 71, 544, 201, 210)],\n",
       "  'top_int': [158, 175, 187, 201],\n",
       "  'content_with_weight': '4.4.2Results\\nAs shown on the right plot of Figure (3), substituting RoPE into Performer leads to rapid convergence and lower loss\\nunder the same amount of training steps. These improvements, in addition to the linear complexity, make Performer\\nmore attractive.',\n",
       "  'content_ltks': '44 2result a shown on the right plot of figur 3 substitut rope into perform lead to rapid converg and lower loss under the same amount of train step these improv in addit to the linear complex make perform more attract',\n",
       "  'content_sm_ltks': '44 2result a shown on the right plot of figur 3 substitut rope into perform lead to rapid converg and lower loss under the same amount of train step these improv in addit to the linear complex make perform more attract'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1412x822>,\n",
       "  'page_num_int': [11, 11, 11],\n",
       "  'position_int': [(11, 70, 541, 224, 235),\n",
       "   (11, 70, 541, 244, 255),\n",
       "   (11, 69, 539, 253, 267)],\n",
       "  'top_int': [224, 244, 253],\n",
       "  'content_with_weight': '4.5Evaluation on Chinese Data\\nIn addition to experiments on English data, we show additional results on Chinese data. To validate the performance of\\nRoFormer on long texts, we conduct experiments on long documents whose length exceeds 512 characters.',\n",
       "  'content_ltks': '4 5evalu on chines data in addit to experi on english data we show addit result on chines data to valid the perform of roform on long text we conduct experi on long document whose length exce 512 charact',\n",
       "  'content_sm_ltks': '4 5evalu on chines data in addit to experi on english data we show addit result on chines data to valid the perform of roform on long text we conduct experi on long document whose length exce 512 charact'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1414x904>,\n",
       "  'page_num_int': [11, 11, 11, 11, 11],\n",
       "  'position_int': [(11, 70, 542, 278, 289),\n",
       "   (11, 71, 543, 296, 307),\n",
       "   (11, 70, 542, 307, 321),\n",
       "   (11, 71, 543, 318, 330),\n",
       "   (11, 71, 543, 330, 340)],\n",
       "  'top_int': [278, 296, 307, 318, 330],\n",
       "  'content_with_weight': '4.5.1 Implementation\\nIn these experiments, we carried out some modifications on WoBERT Su [2020] by replacing the absolute position\\nembedding with our proposed RoPE. As a cross-comparison with other pre-trained Transformer-based models in\\nChinese, i.e. BERT Devlin et al. [2019], WoBERT Su [2020], and NEZHA Wei et al. [2019], we tabulate their\\ntokenization level and position embedding information in Table (3).',\n",
       "  'content_ltks': '45 1 implement in these experi we carri out some modif on wobert su 2020 by replac the absolut posit embed with our propos rope a a cross comparison with other pre train transform base model in chines i e bert devlin et al 2019 wobert su 2020 and nezha wei et al 2019 we tabul their token level and posit embed inform in tabl 3',\n",
       "  'content_sm_ltks': '45 1 implement in these experi we carri out some modif on wobert su 2020 by replac the absolut posit embed with our propos rope a a cross comparison with other pre train transform base model in chines i e bert devlin et al 2019 wobert su 2020 and nezha wei et al 2019 we tabul their token level and posit embed inform in tabl 3'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1414x1042>,\n",
       "  'page_num_int': [11, 11, 11, 11, 11, 11, 11, 11],\n",
       "  'position_int': [(11, 69, 540, 439, 453),\n",
       "   (11, 70, 541, 461, 475),\n",
       "   (11, 71, 543, 473, 485),\n",
       "   (11, 70, 542, 484, 495),\n",
       "   (11, 71, 543, 495, 507),\n",
       "   (11, 70, 542, 506, 518),\n",
       "   (11, 70, 542, 526, 540),\n",
       "   (11, 70, 542, 538, 550)],\n",
       "  'top_int': [439, 461, 473, 484, 495, 506, 526, 538],\n",
       "  'content_with_weight': '4.5.2Pre-training\\nWe pre-train RoFormer on approximately 34GB of data collected from Chinese Wikipedia, news and forums. The\\npre-training is carried out in multiple stages with changing batch size and maximum input sequence length in order to\\nadapt the model to various scenarios. As shown in Table (4), the accuracy of RoFormer elevates with an increasing\\nupper bound of sequence length, which demonstrates the ability of RoFormer in dealing with long texts. We claim that\\nthis is the attribute to the excellent generalizability of the proposed RoPE.\\nTable 4: Pre-training strategy of RoFormer on Chinese dataset. The training procedure is divided into various consecutive\\nstages. In each stage, we train the model with a specific combination of maximum sequence length and batch size.',\n",
       "  'content_ltks': '45 2pre train we pre train roform on approxim 34gb of data collect from chines wikipedia news and forum the pre train is carri out in multipl stage with chang batch size and maximum input sequenc length in order to adapt the model to variou scenario a shown in tabl 4 the accuraci of roform elev with an increas upper bound of sequenc length which demonstr the abil of roform in deal with long text we claim that thi is the attribut to the excel generaliz of the propos rope tabl 4 pre train strategi of roform on chines dataset the train procedur is divid into variou consecut stage in each stage we train the model with a specif combin of maximum sequenc length and batch size',\n",
       "  'content_sm_ltks': '45 2pre train we pre train roform on approxim 34gb of data collect from chines wikipedia news and forum the pre train is carri out in multipl stage with chang batch size and maximum input sequenc length in order to adapt the model to variou scenario a shown in tabl 4 the accuraci of roform elev with an increas upper bound of sequenc length which demonstr the abil of roform in deal with long text we claim that thi is the attribut to the excel generaliz of the propos rope tabl 4 pre train strategi of roform on chines dataset the train procedur is divid into variou consecut stage in each stage we train the model with a specif combin of maximum sequenc length and batch size'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1412x1067>,\n",
       "  'page_num_int': [11, 11, 11, 11, 12, 12, 12, 12, 12],\n",
       "  'position_int': [(11, 70, 541, 662, 674),\n",
       "   (11, 70, 540, 680, 693),\n",
       "   (11, 70, 541, 693, 704),\n",
       "   (11, 84, 554, 711, 723),\n",
       "   (12, 71, 542, 74, 85),\n",
       "   (12, 71, 542, 85, 97),\n",
       "   (12, 71, 542, 96, 108),\n",
       "   (12, 70, 541, 107, 118),\n",
       "   (12, 71, 542, 118, 129)],\n",
       "  'top_int': [662, 680, 693, 711, 74, 85, 96, 107, 118],\n",
       "  'content_with_weight': \"4.5.3Downstream Tasks & Dataset\\nWe choose Chinese AI and Law 2019 Similar Case Matching (CAIL2019-SCM)Xiao et al. [2019] dataset to illustrate\\nthe ability of RoFormer in dealing with long texts, i.e., semantic text matching. CAIL2019-SCM contains 8964 triplets\\n2For this experiment, we adopt code (MIT License) from https : //github. com/lucidrains/performer-pytorch\\nof cases published by the Supreme People's Court of China. The input triplet, denoted as (A, B and C), are fact\\ndescriptions of three cases. The task is to predict whether the pair (A, B) is closer than (A, C) under a predefined\\nsimilarity measure. Note that existing methods mostly cannot perform significantly on CAIL2019-SCM dataset due to\\nthe length of documents (i.e., mostly more than 512 characters). We split train, validation and test sets based on the\\nwell-known ratio 6:2:2.\",\n",
       "  'content_ltks': '45 3downstream task dataset we choos chines ai and law 2019 similar case match cail2019 scm xiao et al 2019 dataset to illustr the abil of roform in deal with long text i e semant text match cail2019 scm contain 8964 triplet 2for thi experi we adopt code mit licens from http github com lucidrain perform pytorch of case publish by the suprem peopl s court of china the input triplet denot a ab and c are fact descript of three case the task is to predict whether the pair ab is closer than a c under a predefin similar measur note that exist method mostli can not perform significantli on cail2019 scm dataset due to the length of document i e mostli more than 512 charact we split train valid and test set base on the well known ratio 6 2 2',\n",
       "  'content_sm_ltks': '45 3downstream task dataset we choos chines ai and law 2019 similar case match cail2019 scm xiao et al 2019 dataset to illustr the abil of roform in deal with long text i e semant text match cail2019 scm contain 8964 triplet 2for thi experi we adopt code mit licens from http github com lucidrain perform pytorch of case publish by the suprem peopl s court of china the input triplet denot a ab and c are fact descript of three case the task is to predict whether the pair ab is closer than a c under a predefin similar measur note that exist method mostli can not perform significantli on cail2019 scm dataset due to the length of document i e mostli more than 512 charact we split train valid and test set base on the well known ratio 6 2 2'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1414x1030>,\n",
       "  'page_num_int': [12, 12, 12, 12, 12, 12, 12, 12],\n",
       "  'position_int': [(12, 71, 543, 141, 152),\n",
       "   (12, 70, 542, 159, 170),\n",
       "   (12, 71, 543, 170, 182),\n",
       "   (12, 71, 543, 181, 193),\n",
       "   (12, 70, 541, 190, 204),\n",
       "   (12, 70, 542, 203, 215),\n",
       "   (12, 70, 542, 222, 234),\n",
       "   (12, 70, 542, 235, 246)],\n",
       "  'top_int': [141, 159, 170, 181, 190, 203, 222, 235],\n",
       "  'content_with_weight': '4.5.4 Results\\nWe apply the pre-trained RoFormer model to CAIL2019-SCM with different input lengths. The model is compared with\\nthe pre-trained BERT and WoBERT model on the same pre-training data, as shown in Table (5). With short text cut-offs,\\ni.e., 512, the result from RoFormer is comparable to WoBERT and is slightly better than the BERT implementation.\\nHowever, when increasing the maximum input text length to 1024, RoFormer outperforms WoBERT by an absolute\\nimprovement of 1.5%.\\nTable 5: Experiment results on CAIL2019-SCM task. Numbers in the first column denote the maximum cut-off\\nsequence length. The results are presented in terms of percent accuracy.',\n",
       "  'content_ltks': '45 4 result we appli the pre train roform model to cail2019 scm with differ input length the model is compar with the pre train bert and wobert model on the same pre train data a shown in tabl 5 with short text cut off i e 512 the result from roform is compar to wobert and is slightli better than the bert implement howev when increas the maximum input text length to 1024 roform outperform wobert by an absolut improv of 15 tabl 5 experi result on cail2019 scm task number in the first column denot the maximum cut off sequenc length the result are present in term of percent accuraci',\n",
       "  'content_sm_ltks': '45 4 result we appli the pre train roform model to cail2019 scm with differ input length the model is compar with the pre train bert and wobert model on the same pre train data a shown in tabl 5 with short text cut off i e 512 the result from roform is compar to wobert and is slightli better than the bert implement howev when increas the maximum input text length to 1024 roform outperform wobert by an absolut improv of 15 tabl 5 experi result on cail2019 scm task number in the first column denot the maximum cut off sequenc length the result are present in term of percent accuraci'},\n",
       " {'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       "  'authors_tks': 'posit embed',\n",
       "  'title_tks': 'roform enhanc transform with rotari',\n",
       "  'title_sm_tks': 'roform enhanc transform with rotari',\n",
       "  'authors_sm_tks': 'posit embed',\n",
       "  'image': <PIL.Image.Image image mode=RGB size=1419x6055>,\n",
       "  'page_num_int': [12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   12,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   13,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14,\n",
       "   14],\n",
       "  'position_int': [(12, 71, 544, 338, 349),\n",
       "   (12, 70, 543, 354, 368),\n",
       "   (12, 70, 543, 367, 378),\n",
       "   (12, 98, 571, 387, 399),\n",
       "   (12, 104, 577, 396, 411),\n",
       "   (12, 106, 579, 410, 421),\n",
       "   (12, 97, 570, 424, 435),\n",
       "   (12, 106, 579, 435, 447),\n",
       "   (12, 107, 580, 447, 458),\n",
       "   (12, 71, 544, 466, 477),\n",
       "   (12, 70, 543, 478, 489),\n",
       "   (12, 68, 541, 501, 515),\n",
       "   (12, 69, 542, 526, 540),\n",
       "   (12, 71, 544, 539, 550),\n",
       "   (12, 71, 544, 550, 561),\n",
       "   (12, 71, 544, 561, 572),\n",
       "   (12, 70, 543, 571, 585),\n",
       "   (12, 70, 543, 580, 594),\n",
       "   (12, 70, 543, 593, 604),\n",
       "   (12, 71, 544, 619, 631),\n",
       "   (12, 69, 542, 637, 651),\n",
       "   (12, 79, 552, 648, 662),\n",
       "   (12, 70, 543, 665, 676),\n",
       "   (12, 80, 553, 675, 686),\n",
       "   (12, 71, 544, 690, 702),\n",
       "   (12, 79, 552, 700, 714),\n",
       "   (12, 80, 553, 712, 724),\n",
       "   (13, 80, 553, 72, 86),\n",
       "   (13, 80, 553, 84, 95),\n",
       "   (13, 70, 543, 99, 111),\n",
       "   (13, 81, 554, 111, 123),\n",
       "   (13, 70, 543, 123, 137),\n",
       "   (13, 80, 553, 136, 147),\n",
       "   (13, 70, 543, 149, 161),\n",
       "   (13, 81, 554, 162, 173),\n",
       "   (13, 80, 553, 171, 183),\n",
       "   (13, 70, 543, 185, 197),\n",
       "   (13, 79, 552, 196, 210),\n",
       "   (13, 80, 553, 209, 221),\n",
       "   (13, 71, 544, 223, 235),\n",
       "   (13, 80, 553, 232, 246),\n",
       "   (13, 70, 543, 246, 260),\n",
       "   (13, 70, 543, 262, 274),\n",
       "   (13, 79, 552, 274, 287),\n",
       "   (13, 69, 542, 286, 300),\n",
       "   (13, 79, 552, 298, 310),\n",
       "   (13, 71, 544, 312, 324),\n",
       "   (13, 79, 552, 322, 336),\n",
       "   (13, 70, 543, 336, 350),\n",
       "   (13, 80, 553, 350, 361),\n",
       "   (13, 70, 543, 364, 376),\n",
       "   (13, 80, 553, 376, 387),\n",
       "   (13, 71, 544, 389, 401),\n",
       "   (13, 79, 552, 398, 412),\n",
       "   (13, 80, 553, 411, 422),\n",
       "   (13, 70, 543, 425, 439),\n",
       "   (13, 70, 543, 439, 451),\n",
       "   (13, 80, 553, 451, 462),\n",
       "   (13, 71, 544, 466, 477),\n",
       "   (13, 81, 554, 477, 488),\n",
       "   (13, 80, 553, 486, 498),\n",
       "   (13, 81, 554, 499, 510),\n",
       "   (13, 71, 544, 513, 524),\n",
       "   (13, 81, 554, 524, 535),\n",
       "   (13, 80, 553, 533, 547),\n",
       "   (13, 80, 553, 543, 557),\n",
       "   (13, 71, 544, 560, 571),\n",
       "   (13, 80, 553, 571, 582),\n",
       "   (13, 80, 553, 581, 593),\n",
       "   (13, 81, 554, 593, 604),\n",
       "   (13, 80, 553, 602, 616),\n",
       "   (13, 70, 543, 616, 630),\n",
       "   (13, 80, 553, 629, 641),\n",
       "   (13, 79, 552, 639, 651),\n",
       "   (13, 70, 543, 652, 666),\n",
       "   (13, 80, 553, 665, 677),\n",
       "   (13, 81, 554, 676, 687),\n",
       "   (13, 69, 542, 689, 703),\n",
       "   (13, 80, 553, 700, 714),\n",
       "   (13, 81, 554, 712, 723),\n",
       "   (14, 72, 545, 74, 85),\n",
       "   (14, 80, 553, 86, 98),\n",
       "   (14, 71, 544, 100, 112),\n",
       "   (14, 79, 552, 109, 123),\n",
       "   (14, 81, 554, 123, 133),\n",
       "   (14, 70, 543, 134, 148),\n",
       "   (14, 80, 553, 146, 160),\n",
       "   (14, 81, 554, 159, 170),\n",
       "   (14, 70, 543, 172, 184),\n",
       "   (14, 80, 553, 185, 194),\n",
       "   (14, 71, 544, 199, 211),\n",
       "   (14, 80, 553, 210, 224),\n",
       "   (14, 70, 543, 224, 236),\n",
       "   (14, 80, 553, 236, 246),\n",
       "   (14, 72, 545, 251, 263),\n",
       "   (14, 81, 554, 263, 274),\n",
       "   (14, 80, 553, 274, 284),\n",
       "   (14, 70, 543, 286, 300),\n",
       "   (14, 70, 543, 301, 315),\n",
       "   (14, 80, 553, 313, 325),\n",
       "   (14, 70, 543, 326, 340),\n",
       "   (14, 80, 553, 338, 352),\n",
       "   (14, 82, 555, 351, 362),\n",
       "   (14, 71, 544, 365, 377),\n",
       "   (14, 80, 553, 377, 387),\n",
       "   (14, 70, 543, 390, 401),\n",
       "   (14, 81, 554, 402, 414),\n",
       "   (14, 70, 543, 414, 428),\n",
       "   (14, 80, 553, 429, 439),\n",
       "   (14, 69, 542, 440, 454),\n",
       "   (14, 80, 553, 454, 466),\n",
       "   (14, 70, 543, 467, 481),\n",
       "   (14, 81, 554, 480, 491),\n",
       "   (14, 71, 544, 494, 505),\n",
       "   (14, 80, 553, 505, 515),\n",
       "   (14, 80, 553, 515, 527),\n",
       "   (14, 80, 553, 526, 538),\n",
       "   (14, 79, 552, 536, 550),\n",
       "   (14, 80, 553, 549, 561),\n",
       "   (14, 71, 544, 564, 576),\n",
       "   (14, 70, 543, 577, 589),\n",
       "   (14, 81, 554, 590, 601),\n",
       "   (14, 72, 545, 604, 616),\n",
       "   (14, 80, 553, 614, 628),\n",
       "   (14, 71, 544, 629, 641),\n",
       "   (14, 81, 554, 642, 653)],\n",
       "  'top_int': [338,\n",
       "   354,\n",
       "   367,\n",
       "   387,\n",
       "   396,\n",
       "   410,\n",
       "   424,\n",
       "   435,\n",
       "   447,\n",
       "   466,\n",
       "   478,\n",
       "   501,\n",
       "   526,\n",
       "   539,\n",
       "   550,\n",
       "   561,\n",
       "   571,\n",
       "   580,\n",
       "   593,\n",
       "   619,\n",
       "   637,\n",
       "   648,\n",
       "   665,\n",
       "   675,\n",
       "   690,\n",
       "   700,\n",
       "   712,\n",
       "   72,\n",
       "   84,\n",
       "   99,\n",
       "   111,\n",
       "   123,\n",
       "   136,\n",
       "   149,\n",
       "   162,\n",
       "   171,\n",
       "   185,\n",
       "   196,\n",
       "   209,\n",
       "   223,\n",
       "   232,\n",
       "   246,\n",
       "   262,\n",
       "   274,\n",
       "   286,\n",
       "   298,\n",
       "   312,\n",
       "   322,\n",
       "   336,\n",
       "   350,\n",
       "   364,\n",
       "   376,\n",
       "   389,\n",
       "   398,\n",
       "   411,\n",
       "   425,\n",
       "   439,\n",
       "   451,\n",
       "   466,\n",
       "   477,\n",
       "   486,\n",
       "   499,\n",
       "   513,\n",
       "   524,\n",
       "   533,\n",
       "   543,\n",
       "   560,\n",
       "   571,\n",
       "   581,\n",
       "   593,\n",
       "   602,\n",
       "   616,\n",
       "   629,\n",
       "   639,\n",
       "   652,\n",
       "   665,\n",
       "   676,\n",
       "   689,\n",
       "   700,\n",
       "   712,\n",
       "   74,\n",
       "   86,\n",
       "   100,\n",
       "   109,\n",
       "   123,\n",
       "   134,\n",
       "   146,\n",
       "   159,\n",
       "   172,\n",
       "   185,\n",
       "   199,\n",
       "   210,\n",
       "   224,\n",
       "   236,\n",
       "   251,\n",
       "   263,\n",
       "   274,\n",
       "   286,\n",
       "   301,\n",
       "   313,\n",
       "   326,\n",
       "   338,\n",
       "   351,\n",
       "   365,\n",
       "   377,\n",
       "   390,\n",
       "   402,\n",
       "   414,\n",
       "   429,\n",
       "   440,\n",
       "   454,\n",
       "   467,\n",
       "   480,\n",
       "   494,\n",
       "   505,\n",
       "   515,\n",
       "   526,\n",
       "   536,\n",
       "   549,\n",
       "   564,\n",
       "   577,\n",
       "   590,\n",
       "   604,\n",
       "   614,\n",
       "   629,\n",
       "   642],\n",
       "  'content_with_weight': '4.5.5Limitations of the work\\nAlthough we provide theoretical groundings as well as promising experimental justifications, our method is limited by\\nfollowing facts:\\n· Despite the fact that we mathematically format the relative position relations as rotations under 2D sub-spaces,\\nthere lacks of thorough explanations on why it converges faster than baseline models that incorporates other\\nposition encoding strategies.\\n· Although we have proved that our model has favourable property of long-term decay for intern-token products,\\nSection (3.3), which is similar to the existing position encoding mechanisms, our model shows superior\\nperformance on long texts than peer models, we have not come up with a faithful explanation.\\nOur proposed RoFormer is built upon the Transformer-based infrastructure, which requires hardware resources for\\npre-trainingpurpose.\\n5Conclusions\\nIn this work, we proposed a new position embedding method that incorporates explicit relative position dependency in\\nself-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative\\nposition can be naturally formulated using vector production in self-attention, with absolution position information\\nbeing encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of\\nthe proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark\\ndatasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show\\nthat our proposed RoFormer can achieve better performance on long texts task.\\nReferences\\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence\\nlearning. In International Conference on Machine Learning, pages 1243-1252. PMLR, 2017.\\nMd. Amirul Islam, Sen Jia, and Neil D. B. Bruce. How much position information do convolutional neural networks\\nencode? ArXiv, abs/2001.08248, 2020.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,\\nvolume 30. Curran Associates, Inc., 2017. URL https: //proceedings.neurips.cc/paper/2017/file/\\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers\\nfor language understanding. In NAACL-HLT, 2019.\\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised\\nmultitask learners. 2019.\\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal\\napproximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2020.\\nURL https: //openreview.net/forum?id=ByxRMoNtvr.\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert:\\nA lite bert for self-supervised learning of language representations. In International Conference on Learning\\nRepresentations, 2020. URL https: //openreview.net/forum?id=H1eA7AEtvS.\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as\\ndiscriminators rather than generators. In ICLR, 2020. URL https : //openreview.net/pdf?id=r1xMH1BtvB.\\nA. Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.\\nAnkur P. Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural\\nlanguage inference. In EMNLP, 2016.\\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In NAACL-HLT,\\n2018.\\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, I. Simon, C. Hawthorne, Andrew M. Dai,\\nM. Hoffman, M. Dinculescu, and D. Eck. Music transformer. arXiv: Learning, 2018.\\nZihang Dai, Z. Yang, Yiming Yang, J. Carbonell, Quoc V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language\\nmodels beyond a fixed-length context. In ACL, 2019.\\nZ. Yang, Zihang Dai, Yiming Yang, J. Carbonell, R. Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive\\npretraining for language understanding. In NeurIPS, 2019.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and\\nPeter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:\\n140:1-140:67, 2020.\\nGuolin Ke, Di He, and T. Liu. Rethinking positional encoding in language pre-training. ArXiv, abs/2006.15595, 2020.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled\\nattention. ArXiv, abs/2006.03654, 2020.\\nZhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better relative position\\nembeddings. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3327-3335, Online,\\nNovember 2020. Ass0ciation for Computational Linguistics. doi:10.18653/v1/2020.findings-emnlp.298. URL\\nhttps: //www.aclweb.org/anthology/2020.findings-emnlp.298.\\nXuanqing Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, and Cho-Jui Hsieh. Learning to encode position for transformer with\\ncontinuous dynamical model. In Proceedings of the 37th International Conference on Machine Learning, ICML\\n2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 6327-6335.\\nPMLR, 2020. URL http: //proceedings.mlr.press/v119/liu20n.html.\\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Samy\\nBengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors,\\nAdvances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing\\nSystems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6572-6583, 2018a. URL https :\\n//proceedings.neurips.cc/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract .html.\\nBenyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. Encoding\\nword order in complex embeddings. In International Conference on Learning Representations, 2020. URL\\nhttps: //openreview.net/forum?id=Hke-WTVtwr.\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregres-\\nsive transformers with linear attention. In International Conference on Machine Learning, pages 5156-5165. PMLR\\n2020.\\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Effcient attention: Attention with linear\\ncomplexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages\\n3531-3539,2021.\\nAmapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and\\nanalysis platform for natural language understanding. 04 2018.\\nKrzysztof Choromanski, Valeri Likhosherstov, David Dohan, Xingyou Song, A. Gane, Tamas Sarlos, Peter Hawkins.\\nJ. Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention\\nwith performers. ArXiv, abs/2009.14794, 2020.\\nOndrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz,\\nPavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Alevs Tamchyna. Findings of the\\n2014 workshop on statistical machine translation. pages 12-58, 06 2014. doi:10.3115/v1/W14-3302.\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. 08\\n2015.\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.\\nfairseq: A fast, extensible toolkit for sequence modeling. pages 48-53, 01 2019. doi:10.18653/v1/N19-4009.\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei Jing Zhu. Bleu: a method for automatic evaluation of machine\\ntranslation. 10 2002. doi:10.3115/1073083.1073135.\\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler\\nAligning books and movies: Towards story-like visual explanations by watching movies and reading books. In arXiv\\npreprint arXiv:1506.06724,2015.\\nWikimedia Foundation. Wikimedia downloads, https : / /dumps . wikimedia. org, 2021.\\nIlya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv e-prints, art. arXiv:1711.05101,\\nNovember 2017.\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceed-\\nings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL https : / /www . ac1web.org/\\nantho1ogy/I05-5002.\\nRichard Socher, A. Perelygin, J.Y. Wu, J. Chuang, C.D. Manning, A.Y. Ng, and C. Potts. Recursive deep models for\\nsemantic compositionality 0ver a sentiment treebank. EMNLP, 1631:1631-1642, 01 2013.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\\ncomprehension of text. pages 2383-2392, 01 2016. doi:10.18653/v1/D16-1264.\\nHussein Al-Natsheh. Udl at semeval-2017 task 1: Semantic textual similarity estimation of english sentence pairs using\\nregression model over pairwise features. 08 2017.\\nZ. Chen, H. Zhang, and L. Zhang, X.and Zhao. Quora question pairs., 2018b. URL https : / /www . kaggle . com/c/\\nquora-question-pairs.\\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding\\nthrough inference. pages 1112-1122, 01 2018. doi:10.18653/v1/N18-1101.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\\nRault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite.\\nJulien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.\\nTransformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association\\nfor Computational Linguistics. URL https: //www.aclweb.org/anthology/2020.emnlp-demos .6.\\nMatt Mahoney. Large text compression benchmark, http: //www .mattmahoney .net/dc/text .html, 2006.\\nJianlin Su. Wobert: Word-based chinese bert model - zhuiyiai. Technical report, 2020. URL https : //github . com/\\nZhuiyiTechnology/WoBERT.\\nVictor Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao\\nChen, and Qun Liu. Nezha: Neural contextualized representation for chinese language understanding. 08 2019.\\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao Tu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang, Xianpei Han\\nZhen hu, Heng Wang, and Jianfeng Xu. Cail2019-scm: A dataset of similar case matching in legal domain. 11 2019.',\n",
       "  'content_ltks': '45 5limit of the work although we provid theoret ground a well a promis experiment justif our method is limit by follow fact despit the fact that we mathemat format the rel posit relat a rotat under 2d sub space there lack of thorough explan on whi it converg faster than baselin model that incorpor other posit encod strategi although we have prove that our model ha favour properti of long term decay for intern token product section 33 which is similar to the exist posit encod mechan our model show superior perform on long text than peer model we have not come up with a faith explan our propos roform is built upon the transform base infrastructur which requir hardwar resourc for pre trainingpurpos 5conclus in thi work we propos a new posit embed method that incorpor explicit rel posit depend in self attent to enhanc the perform of transform architectur our theoret analysi indic that rel posit can be natur formul use vector product in self attent with absolut posit inform be encod through a rotat matrix in addit we mathemat illustr the advantag properti of the propos method when appli to the transform final experi on both english and chines benchmark dataset demonstr that our method encourag faster converg in pre train the experiment result also show that our propos roform can achiev better perform on long text task refer jona gehr michael auli david grangier deni yarat and yann n dauphin convolut sequenc to sequenc learn in intern confer on machin learn page 1243 1252 pmlr 2017 md amirul islam sen jia and neil d b bruce how much posit inform do convolut neural network encod arxiv ab 2001 08248 2020 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jone aidan n gomez l ukasz kaiser and illia polosukhin attent is all you need in i guyon u v luxburg s bengio h wallach r fergu s vishwanathan and r garnett editor advanc in neural inform process system volum 30 curran associ inc 2017 url http proceed neurip cc paper 2017 file 3f5ee243547dee91fbd053c1c4a845aa paper pdf j devlin ming wei chang kenton lee and kristina toutanova bert pre train of deep bidirect transform for languag understand in naacl hlt 2019 a radford jeffrey wu r child david luan dario amodei and ilya sutskev languag model are unsupervis multitask learner 2019 chulhe yun srinadh bhojanap ankit singh rawat sashank reddi and sanjiv kumar are transform univers approxim of sequenc to sequenc function in intern confer on learn represent 2020 url http openreview net forum id byxrmontvr zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma and radu soricut albert a lite bert for self supervis learn of languag represent in intern confer on learn represent 2020 url http openreview net forum id h1ea7aetv kevin clark minh thang luong quoc v le and christoph d man electra pre train text encod a discrimin rather than gener in iclr 2020 url http openreview net pdf id r1xmh1btvb a radford and karthik narasimhan improv languag understand by gener pre train 2018 ankur p parikh oscar tackstrom dipanjan da and jakob uszkoreit a decompos attent model for natur languag infer in emnlp 2016 peter shaw jakob uszkoreit and ashish vaswani self attent with rel posit represent in naacl hlt 2018 cheng zhi anna huang ashish vaswani jakob uszkoreit noam shazeer i simon c hawthorn andrew m dai m hoffman m dinculescu and d eck music transform arxiv learn 2018 zihang dai z yang yime yang j carbonel quoc v le and r salakhutdinov transform xl attent languag model beyond a fix length context in acl 2019 z yang zihang dai yime yang j carbonel r salakhutdinov and quoc v le xlnet gener autoregress pretrain for languag understand in neurip 2019 colin raffel noam shazeer adam robert katherin lee sharan narang michael matena yanqi zhou w li and peter j liu explor the limit of transfer learn with a unifi text to text transform j mach learn re 21 140 1 140 67 2020 guolin ke di he and t liu rethink posit encod in languag pre train arxiv ab 2006 15595 2020 pengcheng he xiaodong liu jianfeng gao and weizhu chen deberta decod enhanc bert with disentangl attent arxiv ab 2006 03654 2020 zhiheng huang davi liang peng xu and bing xiang improv transform model with better rel posit embed in find of the associ for comput linguist emnlp 2020 page 3327 3335 onlin novemb 2020 ass0ciat for comput linguist doi 10 18653 v1 2020 find emnlp 298 url http www aclweb org antholog 2020 find emnlp 298 xuanq liu hsiang fu yu inderjit s dhillon and cho jui hsieh learn to encod posit for transform with continu dynam model in proceed of the 37th intern confer on machin learn icml 2020 13 18 juli 2020 virtual event volum 119 of proceed of machin learn research page 6327 6335 pmlr 2020 url http proceed mlr press v119 liu20n html tian qi chen yulia rubanova jess bettencourt and david duvenaud neural ordinari differenti equat in sami bengio hanna m wallach hugo larochel kristen grauman nicolo cesa bianchi and roman garnett editor advanc in neural inform process system 31 annual confer on neural inform process system 2018 neurip 2018 decemb 38 2018 montr é al canada page 6572 6583 2018a url http proceed neurip cc paper 2018 hash 69386f6bb1dfed68692a24c8686939b9 abstract html benyou wang donghao zhao christina lioma qiuchi li peng zhang and jakob grue simonsen encod word order in complex embed in intern confer on learn represent 2020 url http openreview net forum id hke wtvtwr angelo katharopoulo apoorv vya nikolao pappa and francoi fleuret transform are rnn fast autoregr sive transform with linear attent in intern confer on machin learn page 5156 5165 pmlr 2020 zhuoran shen mingyuan zhang haiyu zhao shuai yi and hongsheng li effcient attent attent with linear complex in proceed of the ieee cvf winter confer on applic of comput vision page 3531 3539 2021 amapreet singh julian michael felix hill omer levi and samuel bowman glue a multi task benchmark and analysi platform for natur languag understand 04 2018 krzysztof choromanski valeri likhosherstov david dohan xingyou song a gane tama sarlo peter hawkin j davi afroz mohiuddin lukasz kaiser david belang luci j colwel and adrian weller rethink attent with perform arxiv ab 2009 14794 2020 ondrej bojar christian buck christian federmann barri haddow philipp koehn johann level christof monz pavel pecina matt post herv saint amand radu soricut lucia specia and alev tamchyna find of the 2014 workshop on statist machin translat page 12 58 06 2014 doi 10 3115 v1 w14 3302 rico sennrich barri haddow and alexandra birch neural machin translat of rare word with subword unit 08 2015 myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and michael auli fairseq a fast extens toolkit for sequenc model page 48 53 01 2019 doi 10 18653 v1 n19 4009 kishor papineni salim rouko todd ward and wei jing zhu bleu a method for automat evalu of machin translat 10 2002 doi 10 3115 1073083 1073135 yukun zhu ryan kiro richard zemel ruslan salakhutdinov raquel urtasun antonio torralba and sanja fidler align book and movi toward stori like visual explan by watch movi and read book in arxiv preprint arxiv 1506 06724 2015 wikimedia foundat wikimedia download http dump wikimedia org 2021 ilya loshchilov and frank hutter decoupl weight decay regular arxiv e print art arxiv 1711 05101 novemb 2017 william b dolan and chri brockett automat construct a corpu of sententi paraphras in proceed ing of the third intern workshop on paraphras iwp2005 2005 url http www ac1web org antho1ogi i05 5002 richard socher a perelygin j y wu j chuang c d man a y ng and c pott recurs deep model for semant composition 0ver a sentiment treebank emnlp 1631 1631 1642 01 2013 pranav rajpurkar jian zhang konstantin lopyrev and perci liang squad 100 000 question for machin comprehens of text page 2383 2392 01 2016 doi 10 18653 v1 d16 1264 hussein al natsheh udl at semev 2017 task 1 semant textual similar estim of english sentenc pair use regress model over pairwis featur 08 2017 z chen h zhang and l zhang x and zhao quora question pair 2018b url http www kaggl com c quora question pair adina william nikita nangia and samuel bowman a broad coverag challeng corpu for sentenc understand through infer page 1112 1122 01 2018 doi 10 18653 v1 n18 1101 thoma wolf lysandr debut victor sanh julien chaumond clement delangu anthoni moi pierric cistac tim rault r é mi louf morgan funtowicz joe davison sam shleifer patrick von platen clara ma yacin jernit julien plu canwen xu teven le scao sylvain gugger mariama drame quentin lhoest and alexand m rush transform state of the art natur languag process in proceed of the 2020 confer on empir method in natur languag process system demonstr page 38 45 onlin octob 2020 associ for comput linguist url http www aclweb org antholog 2020 emnlp demo 6 matt mahoney larg text compress benchmark http www mattmahoney net dc text html 2006 jianlin su wobert word base chines bert model zhuiyiai technic report 2020 url http github com zhuiyitechnolog wobert victor junqiu wei xiaozh ren xiaoguang li wenyong huang yi liao yasheng wang jiashu lin xin jiang xiao chen and qun liu nezha neural contextu represent for chines languag understand 08 2019 chaojun xiao haoxi zhong zhipeng guo cunchao tu zhiyuan liu maosong sun tianyang zhang xianpei han zhen hu heng wang and jianfeng xu cail2019 scm a dataset of similar case match in legal domain 11 2019',\n",
       "  'content_sm_ltks': '45 5limit of the work although we provid theoret ground a well a promis experiment justif our method is limit by follow fact despit the fact that we mathemat format the rel posit relat a rotat under 2d sub space there lack of thorough explan on whi it converg faster than baselin model that incorpor other posit encod strategi although we have prove that our model ha favour properti of long term decay for intern token product section 33 which is similar to the exist posit encod mechan our model show superior perform on long text than peer model we have not come up with a faith explan our propos roform is built upon the transform base infrastructur which requir hardwar resourc for pre trainingpurpos 5conclus in thi work we propos a new posit embed method that incorpor explicit rel posit depend in self attent to enhanc the perform of transform architectur our theoret analysi indic that rel posit can be natur formul use vector product in self attent with absolut posit inform be encod through a rotat matrix in addit we mathemat illustr the advantag properti of the propos method when appli to the transform final experi on both english and chines benchmark dataset demonstr that our method encourag faster converg in pre train the experiment result also show that our propos roform can achiev better perform on long text task refer jona gehr michael auli david grangier deni yarat and yann n dauphin convolut sequenc to sequenc learn in intern confer on machin learn page 1243 1252 pmlr 2017 md amirul islam sen jia and neil d b bruce how much posit inform do convolut neural network encod arxiv ab 2001 08248 2020 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jone aidan n gomez l ukasz kaiser and illia polosukhin attent is all you need in i guyon u v luxburg s bengio h wallach r fergu s vishwanathan and r garnett editor advanc in neural inform process system volum 30 curran associ inc 2017 url http proceed neurip cc paper 2017 file 3f5ee243547dee91fbd053c1c4a845aa paper pdf j devlin ming wei chang kenton lee and kristina toutanova bert pre train of deep bidirect transform for languag understand in naacl hlt 2019 a radford jeffrey wu r child david luan dario amodei and ilya sutskev languag model are unsupervis multitask learner 2019 chulhe yun srinadh bhojanap ankit singh rawat sashank reddi and sanjiv kumar are transform univers approxim of sequenc to sequenc function in intern confer on learn represent 2020 url http openreview net forum id byxrmontvr zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma and radu soricut albert a lite bert for self supervis learn of languag represent in intern confer on learn represent 2020 url http openreview net forum id h1ea7aetv kevin clark minh thang luong quoc v le and christoph d man electra pre train text encod a discrimin rather than gener in iclr 2020 url http openreview net pdf id r1xmh1btvb a radford and karthik narasimhan improv languag understand by gener pre train 2018 ankur p parikh oscar tackstrom dipanjan da and jakob uszkoreit a decompos attent model for natur languag infer in emnlp 2016 peter shaw jakob uszkoreit and ashish vaswani self attent with rel posit represent in naacl hlt 2018 cheng zhi anna huang ashish vaswani jakob uszkoreit noam shazeer i simon c hawthorn andrew m dai m hoffman m dinculescu and d eck music transform arxiv learn 2018 zihang dai z yang yime yang j carbonel quoc v le and r salakhutdinov transform xl attent languag model beyond a fix length context in acl 2019 z yang zihang dai yime yang j carbonel r salakhutdinov and quoc v le xlnet gener autoregress pretrain for languag understand in neurip 2019 colin raffel noam shazeer adam robert katherin lee sharan narang michael matena yanqi zhou w li and peter j liu explor the limit of transfer learn with a unifi text to text transform j mach learn re 21 140 1 140 67 2020 guolin ke di he and t liu rethink posit encod in languag pre train arxiv ab 2006 15595 2020 pengcheng he xiaodong liu jianfeng gao and weizhu chen deberta decod enhanc bert with disentangl attent arxiv ab 2006 03654 2020 zhiheng huang davi liang peng xu and bing xiang improv transform model with better rel posit embed in find of the associ for comput linguist emnlp 2020 page 3327 3335 onlin novemb 2020 ass0ciat for comput linguist doi 10 18653 v1 2020 find emnlp 298 url http www aclweb org antholog 2020 find emnlp 298 xuanq liu hsiang fu yu inderjit s dhillon and cho jui hsieh learn to encod posit for transform with continu dynam model in proceed of the 37th intern confer on machin learn icml 2020 13 18 juli 2020 virtual event volum 119 of proceed of machin learn research page 6327 6335 pmlr 2020 url http proceed mlr press v119 liu20n html tian qi chen yulia rubanova jess bettencourt and david duvenaud neural ordinari differenti equat in sami bengio hanna m wallach hugo larochel kristen grauman nicolo cesa bianchi and roman garnett editor advanc in neural inform process system 31 annual confer on neural inform process system 2018 neurip 2018 decemb 38 2018 montr é al canada page 6572 6583 2018a url http proceed neurip cc paper 2018 hash 69386f6bb1dfed68692a24c8686939b9 abstract html benyou wang donghao zhao christina lioma qiuchi li peng zhang and jakob grue simonsen encod word order in complex embed in intern confer on learn represent 2020 url http openreview net forum id hke wtvtwr angelo katharopoulo apoorv vya nikolao pappa and francoi fleuret transform are rnn fast autoregr sive transform with linear attent in intern confer on machin learn page 5156 5165 pmlr 2020 zhuoran shen mingyuan zhang haiyu zhao shuai yi and hongsheng li effcient attent attent with linear complex in proceed of the ieee cvf winter confer on applic of comput vision page 3531 3539 2021 amapreet singh julian michael felix hill omer levi and samuel bowman glue a multi task benchmark and analysi platform for natur languag understand 04 2018 krzysztof choromanski valeri likhosherstov david dohan xingyou song a gane tama sarlo peter hawkin j davi afroz mohiuddin lukasz kaiser david belang luci j colwel and adrian weller rethink attent with perform arxiv ab 2009 14794 2020 ondrej bojar christian buck christian federmann barri haddow philipp koehn johann level christof monz pavel pecina matt post herv saint amand radu soricut lucia specia and alev tamchyna find of the 2014 workshop on statist machin translat page 12 58 06 2014 doi 10 3115 v1 w14 3302 rico sennrich barri haddow and alexandra birch neural machin translat of rare word with subword unit 08 2015 myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and michael auli fairseq a fast extens toolkit for sequenc model page 48 53 01 2019 doi 10 18653 v1 n19 4009 kishor papineni salim rouko todd ward and wei jing zhu bleu a method for automat evalu of machin translat 10 2002 doi 10 3115 1073083 1073135 yukun zhu ryan kiro richard zemel ruslan salakhutdinov raquel urtasun antonio torralba and sanja fidler align book and movi toward stori like visual explan by watch movi and read book in arxiv preprint arxiv 1506 06724 2015 wikimedia foundat wikimedia download http dump wikimedia org 2021 ilya loshchilov and frank hutter decoupl weight decay regular arxiv e print art arxiv 1711 05101 novemb 2017 william b dolan and chri brockett automat construct a corpu of sententi paraphras in proceed ing of the third intern workshop on paraphras iwp2005 2005 url http www ac1web org antho1ogi i05 5002 richard socher a perelygin j y wu j chuang c d man a y ng and c pott recurs deep model for semant composition 0ver a sentiment treebank emnlp 1631 1631 1642 01 2013 pranav rajpurkar jian zhang konstantin lopyrev and perci liang squad 100 000 question for machin comprehens of text page 2383 2392 01 2016 doi 10 18653 v1 d16 1264 hussein al natsheh udl at semev 2017 task 1 semant textual similar estim of english sentenc pair use regress model over pairwis featur 08 2017 z chen h zhang and l zhang x and zhao quora question pair 2018b url http www kaggl com c quora question pair adina william nikita nangia and samuel bowman a broad coverag challeng corpu for sentenc understand through infer page 1112 1122 01 2018 doi 10 18653 v1 n18 1101 thoma wolf lysandr debut victor sanh julien chaumond clement delangu anthoni moi pierric cistac tim rault r é mi louf morgan funtowicz joe davison sam shleifer patrick von platen clara ma yacin jernit julien plu canwen xu teven le scao sylvain gugger mariama drame quentin lhoest and alexand m rush transform state of the art natur languag process in proceed of the 2020 confer on empir method in natur languag process system demonstr page 38 45 onlin octob 2020 associ for comput linguist url http www aclweb org antholog 2020 emnlp demo 6 matt mahoney larg text compress benchmark http www mattmahoney net dc text html 2006 jianlin su wobert word base chines bert model zhuiyiai technic report 2020 url http github com zhuiyitechnolog wobert victor junqiu wei xiaozh ren xiaoguang li wenyong huang yi liao yasheng wang jiashu lin xin jiang xiao chen and qun liu nezha neural contextu represent for chines languag understand 08 2019 chaojun xiao haoxi zhong zhipeng guo cunchao tu zhiyuan liu maosong sun tianyang zhang xianpei han zhen hu heng wang and jianfeng xu cail2019 scm a dataset of similar case match in legal domain 11 2019'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d416508c-4163-4b06-9ddc-acabeda05f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "image = chunks[15]['image']  #Image.open(\"example.jpg\")  # 替换为你的图片路径\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "186209e9-97a1-434d-b989-adb7dd6d241d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docnm_kwd': '/root/docs//rope旋转位置编码.pdf',\n",
       " 'authors_tks': 'posit embed',\n",
       " 'title_tks': 'roform enhanc transform with rotari',\n",
       " 'title_sm_tks': 'roform enhanc transform with rotari',\n",
       " 'authors_sm_tks': 'posit embed',\n",
       " 'content_with_weight': \"Figure 1: Implementation of Rotary Position Embedding(RoPE).\\nm0\\nx'2\\nX2\\n(X1,X2)\\n(x'1,x'2)\\nQuery / Key\\nx'1\\nX1\\nm.\\nd=2\\n02\\n..\\nEnhanced\\nTransforr\\nrmer\\nwith\\n··\\nRotary …·\\n··\\nPosition\\nEmbedding\\n：：\\n6\\nPosition\\nPosition Er\\ncodedQuery/Key\\n/Key\",\n",
       " 'content_ltks': 'figur 1 implement of rotari posit embed rope m0 x 2 x2 x1 x2 x 1x 2 queri key x 1 x1 m d2 02 enhanc transforr rmer with rotari posit embed 6 posit posit er codedqueri key key',\n",
       " 'content_sm_ltks': 'figur 1 implement of rotari posit embed rope m0 x 2 x2 x1 x2 x 1x 2 queri key x 1 x1 m d2 02 enhanc transforr rmer with rotari posit embed 6 posit posit er codedqueri key key',\n",
       " 'image': <PIL.Image.Image image mode=RGB size=993x555>,\n",
       " 'doc_type_kwd': 'image',\n",
       " 'page_num_int': [5],\n",
       " 'position_int': [(5, 139, 470, 402, 587)],\n",
       " 'top_int': [402]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

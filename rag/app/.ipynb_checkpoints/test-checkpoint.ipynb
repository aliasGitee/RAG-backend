{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2eb6a1-e737-4da4-a2c8-67e7a6369d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[0;93m2025-07-15 15:35:41.311055205 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-07-15 15:35:41.311097628 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    readed = [0] * len(paper[\"lines\"])\\n    # find colon firstly\\n    i = 0\\n    while i + 1 < len(paper[\"lines\"]):\\n        txt = pdf_parser.remove_tag(paper[\"lines\"][i][0])\\n        j = i\\n        if txt.strip(\"\\n\").strip()[-1] not in \":：\":\\n            i += 1\\n            continue\\n        i += 1\\n        while i < len(paper[\"lines\"]) and not paper[\"lines\"][i][0]:\\n            i += 1\\n        if i >= len(paper[\"lines\"]): break\\n        proj = [paper[\"lines\"][i][0].strip()]\\n        i += 1\\n        while i < len(paper[\"lines\"]) and paper[\"lines\"][i][0].strip()[0] == proj[-1][0]:\\n            proj.append(paper[\"lines\"][i])\\n            i += 1\\n        for k in range(j, i): readed[k] = True\\n        txt = txt[::-1]\\n        if eng:\\n            r = re.search(r\"(.*?) ([\\\\.;?!]|$)\", txt)\\n            txt = r.group(1)[::-1] if r else txt[::-1]\\n        else:\\n            r = re.search(r\"(.*?) ([。？；！]|$)\", txt)\\n            txt = r.group(1)[::-1] if r else txt[::-1]\\n        for p in proj:\\n            d = copy.deepcopy(doc)\\n            txt += \"\\n\" + pdf_parser.remove_tag(p)\\n            d[\"image\"], poss = pdf_parser.crop(p, need_position=True)\\n            add_positions(d, poss)\\n            tokenize(d, txt, eng)\\n            res.append(d)\\n\\n    i = 0\\n    chunk = []\\n    tk_cnt = 0\\n    def add_chunk():\\n        nonlocal chunk, res, doc, pdf_parser, tk_cnt\\n        d = copy.deepcopy(doc)\\n        ck = \"\\n\".join(chunk)\\n        tokenize(d, pdf_parser.remove_tag(ck), pdf_parser.is_english)\\n        d[\"image\"], poss = pdf_parser.crop(ck, need_position=True)\\n        add_positions(d, poss)\\n        res.append(d)\\n        chunk = []\\n        tk_cnt = 0\\n\\n    while i < len(paper[\"lines\"]):\\n        if tk_cnt > 128:\\n            add_chunk()\\n        if readed[i]:\\n            i += 1\\n            continue\\n        readed[i] = True\\n        txt, layouts = paper[\"lines\"][i]\\n        txt_ = pdf_parser.remove_tag(txt)\\n        i += 1\\n        cnt = num_tokens_from_string(txt_)\\n        if any([\\n            layouts.find(\"title\") >= 0 and chunk,\\n            cnt + tk_cnt > 128 and tk_cnt > 32,\\n        ]):\\n            add_chunk()\\n            chunk = [txt]\\n            tk_cnt = cnt\\n        else:\\n            chunk.append(txt)\\n            tk_cnt += cnt\\n\\n    if chunk: add_chunk()\\n    for i, d in enumerate(res):\\n        print(d)\\n        # d[\"image\"].save(f\"./logs/{i}.jpg\")\\n    return res\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "#  Copyright 2025 The InfiniFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "#\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "# sys.path.append(\"/root/ragflow\")\n",
    "from api.db import ParserType\n",
    "from rag.nlp import rag_tokenizer, tokenize, tokenize_table, add_positions, bullets_category, title_frequency, tokenize_chunks\n",
    "from deepdoc.parser import PdfParser, PlainParser\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Pdf(PdfParser):\n",
    "    def __init__(self):\n",
    "        self.model_speciess = ParserType.PAPER.value\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, filename, binary=None, from_page=0,\n",
    "                 to_page=100000, zoomin=3, callback=None):\n",
    "        from timeit import default_timer as timer\n",
    "        start = timer()\n",
    "        callback(msg=\"OCR started\")\n",
    "        self.__images__(\n",
    "            filename if not binary else binary,\n",
    "            zoomin,\n",
    "            from_page,\n",
    "            to_page,\n",
    "            callback\n",
    "        )\n",
    "        callback(msg=\"OCR finished ({:.2f}s)\".format(timer() - start))\n",
    "\n",
    "        start = timer()\n",
    "        self._layouts_rec(zoomin)\n",
    "        callback(0.63, \"Layout analysis ({:.2f}s)\".format(timer() - start))\n",
    "        logging.debug(f\"layouts cost: {timer() - start}s\")\n",
    "\n",
    "        start = timer()\n",
    "        self._table_transformer_job(zoomin)\n",
    "        callback(0.68, \"Table analysis ({:.2f}s)\".format(timer() - start))\n",
    "\n",
    "        start = timer()\n",
    "        self._text_merge()\n",
    "        tbls = self._extract_table_figure(True, zoomin, True, True)\n",
    "        column_width = np.median([b[\"x1\"] - b[\"x0\"] for b in self.boxes])\n",
    "        self._concat_downward()\n",
    "        self._filter_forpages()\n",
    "        callback(0.75, \"Text merged ({:.2f}s)\".format(timer() - start))\n",
    "\n",
    "        # clean mess\n",
    "        if column_width < self.page_images[0].size[0] / zoomin / 2:\n",
    "            logging.debug(\"two_column................... {} {}\".format(column_width,\n",
    "                  self.page_images[0].size[0] / zoomin / 2))\n",
    "            self.boxes = self.sort_X_by_page(self.boxes, column_width / 2)\n",
    "        for b in self.boxes:\n",
    "            b[\"text\"] = re.sub(r\"([\\t 　]|\\u3000){2,}\", \" \", b[\"text\"].strip())\n",
    "\n",
    "        def _begin(txt):\n",
    "            return re.match(\n",
    "                \"[0-9. 一、i]*(introduction|abstract|摘要|引言|keywords|key words|关键词|background|背景|目录|前言|contents)\",\n",
    "                txt.lower().strip())\n",
    "\n",
    "        if from_page > 0:\n",
    "            return {\n",
    "                \"title\": \"\",\n",
    "                \"authors\": \"\",\n",
    "                \"abstract\": \"\",\n",
    "                \"sections\": [(b[\"text\"] + self._line_tag(b, zoomin), b.get(\"layoutno\", \"\")) for b in self.boxes if\n",
    "                             re.match(r\"(text|title)\", b.get(\"layoutno\", \"text\"))],\n",
    "                \"tables\": tbls\n",
    "            }\n",
    "        # get title and authors\n",
    "        title = \"\"\n",
    "        authors = []\n",
    "        i = 0\n",
    "        while i < min(32, len(self.boxes)-1):\n",
    "            b = self.boxes[i]\n",
    "            i += 1\n",
    "            if b.get(\"layoutno\", \"\").find(\"title\") >= 0:\n",
    "                title = b[\"text\"]\n",
    "                if _begin(title):\n",
    "                    title = \"\"\n",
    "                    break\n",
    "                for j in range(3):\n",
    "                    if _begin(self.boxes[i + j][\"text\"]):\n",
    "                        break\n",
    "                    authors.append(self.boxes[i + j][\"text\"])\n",
    "                    break\n",
    "                break\n",
    "        # get abstract\n",
    "        abstr = \"\"\n",
    "        i = 0\n",
    "        while i + 1 < min(32, len(self.boxes)):\n",
    "            b = self.boxes[i]\n",
    "            i += 1\n",
    "            txt = b[\"text\"].lower().strip()\n",
    "            if re.match(\"(abstract|摘要)\", txt):\n",
    "                if len(txt.split()) > 32 or len(txt) > 64:\n",
    "                    abstr = txt + self._line_tag(b, zoomin)\n",
    "                    break\n",
    "                txt = self.boxes[i][\"text\"].lower().strip()\n",
    "                if len(txt.split()) > 32 or len(txt) > 64:\n",
    "                    abstr = txt + self._line_tag(self.boxes[i], zoomin)\n",
    "                i += 1\n",
    "                break\n",
    "        if not abstr:\n",
    "            i = 0\n",
    "\n",
    "        callback(\n",
    "            0.8, \"Page {}~{}: Text merging finished\".format(\n",
    "                from_page, min(\n",
    "                    to_page, self.total_page)))\n",
    "        for b in self.boxes:\n",
    "            logging.debug(\"{} {}\".format(b[\"text\"], b.get(\"layoutno\")))\n",
    "        logging.debug(\"{}\".format(tbls))\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"authors\": \" \".join(authors),\n",
    "            \"abstract\": abstr,\n",
    "            \"sections\": [(b[\"text\"] + self._line_tag(b, zoomin), b.get(\"layoutno\", \"\")) for b in self.boxes[i:] if\n",
    "                         re.match(r\"(text|title)\", b.get(\"layoutno\", \"text\"))],\n",
    "            \"tables\": tbls\n",
    "        }\n",
    "\n",
    "\n",
    "def chunk(filename, binary=None, from_page=0, to_page=100000,\n",
    "          lang=\"Chinese\", callback=None, **kwargs):\n",
    "    \"\"\"\n",
    "        Only pdf is supported.\n",
    "        The abstract of the paper will be sliced as an entire chunk, and will not be sliced partly.\n",
    "    \"\"\"\n",
    "    if re.search(r\"\\.pdf$\", filename, re.IGNORECASE):\n",
    "        if kwargs.get(\"parser_config\", {}).get(\"layout_recognize\", \"DeepDOC\") == \"Plain Text\":\n",
    "            pdf_parser = PlainParser()\n",
    "            paper = {\n",
    "                \"title\": filename,\n",
    "                \"authors\": \" \",\n",
    "                \"abstract\": \"\",\n",
    "                \"sections\": pdf_parser(filename if not binary else binary, from_page=from_page, to_page=to_page)[0],\n",
    "                \"tables\": []\n",
    "            }\n",
    "        else:\n",
    "            pdf_parser = Pdf()\n",
    "            paper = pdf_parser(filename if not binary else binary,\n",
    "                               from_page=from_page, to_page=to_page, callback=callback)\n",
    "    else:\n",
    "        raise NotImplementedError(\"file type not supported yet(pdf supported)\")\n",
    "\n",
    "    doc = {\"docnm_kwd\": filename, \"authors_tks\": rag_tokenizer.tokenize(paper[\"authors\"]),\n",
    "           \"title_tks\": rag_tokenizer.tokenize(paper[\"title\"] if paper[\"title\"] else filename)}\n",
    "    doc[\"title_sm_tks\"] = rag_tokenizer.fine_grained_tokenize(doc[\"title_tks\"])\n",
    "    doc[\"authors_sm_tks\"] = rag_tokenizer.fine_grained_tokenize(doc[\"authors_tks\"])\n",
    "    # is it English\n",
    "    eng = lang.lower() == \"english\"  # pdf_parser.is_english\n",
    "    logging.debug(\"It's English.....{}\".format(eng))\n",
    "\n",
    "    res = tokenize_table(paper[\"tables\"], doc, eng)\n",
    "\n",
    "    if paper[\"abstract\"]:\n",
    "        d = copy.deepcopy(doc)\n",
    "        txt = pdf_parser.remove_tag(paper[\"abstract\"])\n",
    "        d[\"important_kwd\"] = [\"abstract\", \"总结\", \"概括\", \"summary\", \"summarize\"]\n",
    "        d[\"important_tks\"] = \" \".join(d[\"important_kwd\"])\n",
    "        d[\"image\"], poss = pdf_parser.crop(\n",
    "            paper[\"abstract\"], need_position=True)\n",
    "        add_positions(d, poss)\n",
    "        tokenize(d, txt, eng)\n",
    "        res.append(d)\n",
    "\n",
    "    sorted_sections = paper[\"sections\"]\n",
    "    # set pivot using the most frequent type of title,\n",
    "    # then merge between 2 pivot\n",
    "    bull = bullets_category([txt for txt, _ in sorted_sections])\n",
    "    most_level, levels = title_frequency(bull, sorted_sections)\n",
    "    assert len(sorted_sections) == len(levels)\n",
    "    sec_ids = []\n",
    "    sid = 0\n",
    "    for i, lvl in enumerate(levels):\n",
    "        if lvl <= most_level and i > 0 and lvl != levels[i - 1]:\n",
    "            sid += 1\n",
    "        sec_ids.append(sid)\n",
    "        logging.debug(\"{} {} {} {}\".format(lvl, sorted_sections[i][0], most_level, sid))\n",
    "\n",
    "    chunks = []\n",
    "    last_sid = -2\n",
    "    for (txt, _), sec_id in zip(sorted_sections, sec_ids):\n",
    "        if sec_id == last_sid:\n",
    "            if chunks:\n",
    "                chunks[-1] += \"\\n\" + txt\n",
    "                continue\n",
    "        chunks.append(txt)\n",
    "        last_sid = sec_id\n",
    "    res.extend(tokenize_chunks(chunks, doc, eng, pdf_parser))\n",
    "    return res\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    readed = [0] * len(paper[\"lines\"])\n",
    "    # find colon firstly\n",
    "    i = 0\n",
    "    while i + 1 < len(paper[\"lines\"]):\n",
    "        txt = pdf_parser.remove_tag(paper[\"lines\"][i][0])\n",
    "        j = i\n",
    "        if txt.strip(\"\\n\").strip()[-1] not in \":：\":\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        while i < len(paper[\"lines\"]) and not paper[\"lines\"][i][0]:\n",
    "            i += 1\n",
    "        if i >= len(paper[\"lines\"]): break\n",
    "        proj = [paper[\"lines\"][i][0].strip()]\n",
    "        i += 1\n",
    "        while i < len(paper[\"lines\"]) and paper[\"lines\"][i][0].strip()[0] == proj[-1][0]:\n",
    "            proj.append(paper[\"lines\"][i])\n",
    "            i += 1\n",
    "        for k in range(j, i): readed[k] = True\n",
    "        txt = txt[::-1]\n",
    "        if eng:\n",
    "            r = re.search(r\"(.*?) ([\\\\.;?!]|$)\", txt)\n",
    "            txt = r.group(1)[::-1] if r else txt[::-1]\n",
    "        else:\n",
    "            r = re.search(r\"(.*?) ([。？；！]|$)\", txt)\n",
    "            txt = r.group(1)[::-1] if r else txt[::-1]\n",
    "        for p in proj:\n",
    "            d = copy.deepcopy(doc)\n",
    "            txt += \"\\n\" + pdf_parser.remove_tag(p)\n",
    "            d[\"image\"], poss = pdf_parser.crop(p, need_position=True)\n",
    "            add_positions(d, poss)\n",
    "            tokenize(d, txt, eng)\n",
    "            res.append(d)\n",
    "\n",
    "    i = 0\n",
    "    chunk = []\n",
    "    tk_cnt = 0\n",
    "    def add_chunk():\n",
    "        nonlocal chunk, res, doc, pdf_parser, tk_cnt\n",
    "        d = copy.deepcopy(doc)\n",
    "        ck = \"\\n\".join(chunk)\n",
    "        tokenize(d, pdf_parser.remove_tag(ck), pdf_parser.is_english)\n",
    "        d[\"image\"], poss = pdf_parser.crop(ck, need_position=True)\n",
    "        add_positions(d, poss)\n",
    "        res.append(d)\n",
    "        chunk = []\n",
    "        tk_cnt = 0\n",
    "\n",
    "    while i < len(paper[\"lines\"]):\n",
    "        if tk_cnt > 128:\n",
    "            add_chunk()\n",
    "        if readed[i]:\n",
    "            i += 1\n",
    "            continue\n",
    "        readed[i] = True\n",
    "        txt, layouts = paper[\"lines\"][i]\n",
    "        txt_ = pdf_parser.remove_tag(txt)\n",
    "        i += 1\n",
    "        cnt = num_tokens_from_string(txt_)\n",
    "        if any([\n",
    "            layouts.find(\"title\") >= 0 and chunk,\n",
    "            cnt + tk_cnt > 128 and tk_cnt > 32,\n",
    "        ]):\n",
    "            add_chunk()\n",
    "            chunk = [txt]\n",
    "            tk_cnt = cnt\n",
    "        else:\n",
    "            chunk.append(txt)\n",
    "            tk_cnt += cnt\n",
    "\n",
    "    if chunk: add_chunk()\n",
    "    for i, d in enumerate(res):\n",
    "        print(d)\n",
    "        # d[\"image\"].save(f\"./logs/{i}.jpg\")\n",
    "    return res\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c6235-eb4a-4d05-9dcc-8aebc2692d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_paths= ['/root/docs//rope旋转位置编码.pdf']\n",
      "/root/docs//rope旋转位置编码.pdf\n",
      "第0path= /root/docs//rope旋转位置编码.pdf\n",
      "None OCR started\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def dummy(prog=None, msg=\"\"):\n",
    "    print(prog,msg)\n",
    "#chunk(sys.argv[1], callback=dummy)\n",
    "import os\n",
    "path = \"/root/docs/\"\n",
    "files=os.listdir(path)\n",
    "file_paths=[path+\"/\"+file for file in files]\n",
    "print(\"file_paths=\",file_paths)\n",
    "for i in file_paths:\n",
    "    print(i)\n",
    "all_chunks = []\n",
    "# 遍历处理每个文件\n",
    "for i,path in enumerate(file_paths):\n",
    "    print(f\"第{i}path=\",path)\n",
    "    chunks = chunk(path, callback=dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e79d7-3e21-43f6-9d48-c756544ff0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
